{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "provenance": [],
      "toc_visible": true,
      "authorship_tag": "ABX9TyM4TprvGhTx6ZKCtfw7O+qG",
      "include_colab_link": true
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "view-in-github",
        "colab_type": "text"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/ba-careerpath-7/Customer_Transaction_A-B_Testing_Project_KAGGLE_VERSION/blob/main/Data_Driven_E_Commerce_A_B_Testing_Project_KAGGLE_VERSION_2_0.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "# âœŒðŸ¼ Data Driven E-Commerce A/B Testing Project\n",
        "\n"
      ],
      "metadata": {
        "id": "p1sr9vSXTBAE"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "#â­ SECTION A: Introduction\n",
        "\n",
        "Hello guys!\n",
        "\n",
        "Welcome to the A/B Testing Project!\n",
        "\n",
        "\n",
        "Many people who are into experiments such as marketing data may have already heard the phrase \"A/B Testing\". This may sound like a complex topic, but I am here to tell you that is NOT a complex topic!\n",
        "\n",
        "If you are familiar with statistics, A/B Testing is just a two sample statistical test in disguise, such as a two sample t-test. That's it!\n",
        "\n",
        "So this project will go over the basics of statistical inference, A/B testing, and even applications on how to implement Natural Language Processing (NLP) and bootstrapping. We will even extend A/B testing to A/B/C/D/E testing using Analysis of Variance (ANOVA).\n",
        "\n",
        "\n",
        "\n",
        "---\n",
        "\n",
        "\n",
        "\n"
      ],
      "metadata": {
        "id": "KrMNRQupTGkb"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "### Motivation for doing statistical tests such as A/B tests!\n",
        "\n",
        "Lets say that we conduct a A/B test on the weights of NBA centers vs NBA point guards. We find that we have evidence that the weights of centers and point guards are different.\n",
        "\n",
        "We can use that decision to make basketball strategies where a center who tends to be heavier should prioritize being in the paint, while a point guard should do plays that require fast movement since point guards tend to weigh less.\n",
        "\n",
        "Statistical tests like t-tests, A/B tests, and ANOVA are a powerful tools for data-driven decision making. This project shows some reasons why they are widely used in data science today.\n",
        "\n",
        "---"
      ],
      "metadata": {
        "id": "BwDO8JXHNape"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "### What questions are we examining?\n",
        "\n",
        "This project will use customer transaction data. This data contains 10,000 customers with variables such as age, gender, country, annual income, number of purchases, feedback ratings, and churn rates.\n",
        "\n",
        "* Variables such as age, gender, and country will be our groups that we compare. Notice that these variables are discrete numbers or categorical types.\n",
        "\n",
        "* Variables such as annual income, number of purchases, and churn rates are continuous numbers that can be decimals. We will measure how groups differ from each other using these continuous variables. (Notice that feedback ratings are words, not numbers. In SECTION F, we will go over how to use NLP to get numerical scores out of those sentences.)\n",
        "\n",
        "\n",
        "**We will answer 6 questions in this project:**\n",
        "\n",
        "* Question 1. Single Mean Test on annual income for USA customers:\n",
        "\n",
        "Does this data set show a statistically different average from the USA average annual income?\n",
        "\n",
        "* Question 2. Difference in Means A/B Test on annual income for Males and Females:\n",
        "\n",
        "Do we have evidence that males and females have different average annual incomes?\n",
        "\n",
        "\n",
        "* Question 3. Difference in Proportions A/B Test on churned customers for USA and other countries:\n",
        "\n",
        "Do we have evidence that the proportion of USA churned customers is different from the proportion of churned customers from other countries?\n",
        "\n",
        "(Churned definition: To stop using a product or service.)\n",
        "\n",
        "\n",
        "* Question 4. Difference in Means A/B Test on feedback text for young and elder people (Using NLP):\n",
        "\n",
        "Can we find evidence that young and elder people have different average emotional scores when leaving feedback for the products they buy?\n",
        "\n",
        "* Question 5. One Way ANOVA Test on the average number of purchases made for each of the 5 countries:\n",
        "\n",
        "Is there evidence that at LEAST one country has a different average number of purchases when analyzing 5 different countries?\n",
        "\n",
        "* Question 6. Difference in Means A/B Test on annual income for Males and Females (Bootstrap Resampling to revisit Question 2):\n",
        "\n",
        "Can we give additional evidence of Question 2's result? We will use a process called bootstrapping that mimics real data, even if our sample size is small.\n",
        "\n",
        "Before answering these questions, lets jump into cleaning and exploring the data!!!\n",
        "\n",
        "---"
      ],
      "metadata": {
        "id": "V9mp6A1nOO5e"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# Regular data science starting imports:\n",
        "\n",
        "import numpy as np\n",
        "import pandas as pd\n",
        "import matplotlib.pyplot as plt\n",
        "import seaborn as sns\n",
        "\n",
        "import warnings\n",
        "warnings.filterwarnings(\"ignore\") # will remove red warnings in final output\n",
        "\n",
        "sns.set()"
      ],
      "metadata": {
        "id": "T7N93FAQTHIi"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# starting imports for A/B testing:\n",
        "\n",
        "# ALERT: We need this to do statistical inference!\n",
        "\n",
        "from scipy import stats\n",
        "\n",
        "\n",
        "\n"
      ],
      "metadata": {
        "id": "Gk53XyRzTi9t"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "## How to Navigate this Project\n",
        "\n",
        "This project contains a lot of content, so I decided to put emojis on each major section and on summary tables.\n",
        "\n",
        "\n",
        "Here is the content table below:"
      ],
      "metadata": {
        "id": "x7lW1__bmfOZ"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "content_table = pd.DataFrame({\"Major Sections and Table Results\":[\n",
        "    \"SECTION A: Introduction\",\n",
        "    \"SECTION B: Data Cleaning, Data Visualization, and Additional Discoveries\",\n",
        "    \"SECTION C: Exploratory Data Analysis (EDA)\",\n",
        "    \"SECTION D: Hypothesis Tests, Confidence Intervals, Rejection Regions, and p-values\",\n",
        "    \"SECTION E: Diving into A/B Testing\",\n",
        "    \"SECTION F: Combining Natural Language Processing (NLP) and A/B Testing\",\n",
        "    \"SECTION G: ANOVA, A/B/C/D/E... Testing\",\n",
        "    \"SECTION H: Statistical Inference on Bootstrapping\",\n",
        "    \"SECTION I: A/B testing on Bootstrapped data\",\n",
        "    \"SECTION J: Final Conclusion\",\n",
        "    \"Key Considerations\"],\n",
        "\n",
        "\n",
        "                              \"Associated Emoji\":[\"â­\",\n",
        "                                                  \"ðŸ“Š\",\n",
        "                                                  \"ðŸ”Ž\",\n",
        "                                                  \"ðŸ“\",\n",
        "                                                  \"ðŸ‘¨â€ðŸ‘¦\",\n",
        "                                                  \"ðŸŒ´\",\n",
        "                                                  \"ðŸ”¢\",\n",
        "                                                  \"ðŸ‘¢\",\n",
        "                                                  \"ðŸ«\",\n",
        "                                                  \"ðŸ’¡\",\n",
        "                                                  \"â—\"\n",
        "                                                  ]})\n",
        "\n",
        "content_table"
      ],
      "metadata": {
        "id": "OADpFZEMmjSr"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "## Uploading the CSV file:\n",
        "\n",
        "I got my CSV file from this Kaggle Website:\n",
        "https://www.kaggle.com/datasets/fares279/customers-transactions/data\n",
        "\n"
      ],
      "metadata": {
        "id": "T-lS6arBUocV"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "# Here is the A/B Customer Transaction data set:\n"
      ],
      "metadata": {
        "id": "Y_l9XDRlUoeg"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "#---------------------------------------------------------------------\n",
        "# code that allows us to import files from computer to google collab\n",
        "from google.colab import files\n",
        "import io\n",
        "\n",
        "# step 1: upload your CSV (colab will show a file picker)\n",
        "uploaded = files.upload()\n",
        "\n",
        "# step 2: read the uploaded CSV into a pandas DataFrame\n",
        "\n",
        "# SELECT: customer_transaction_AB_project from files\n",
        "\n",
        "\n",
        "AB_df = pd.read_csv(io.BytesIO(list(uploaded.values())[0]))\n",
        "\n",
        "AB_df\n",
        "\n",
        "\n",
        "#---------------------------------------------------------------------\n",
        "\n",
        "\n"
      ],
      "metadata": {
        "id": "Pop2TSrxI1BS"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "## What each variable does:\n",
        "\n",
        "According to Kacem's Customer Transaction data set from Kaggle,\n",
        "\n",
        "\"\n",
        "* customer_id â€” Unique user identifier.\n",
        "\n",
        "* age â€” Age of the customer.\n",
        "\n",
        "* gender â€” Customer gender (Male/Female).\n",
        "\n",
        "* country â€” Country of origin.\n",
        "\n",
        "* annual_income â€” Yearly income in USD.\n",
        "\n",
        "* spending_score â€” Customer loyalty/spending rating (1â€“100).\n",
        "\n",
        "* num_purchases â€” Total purchases made.\n",
        "\n",
        "* avg_purchase_value â€” Average order value.\n",
        "\n",
        "* membership_years â€” Years as a member.\n",
        "\n",
        "* website_visits_per_month â€” Average site visits per month.\n",
        "\n",
        "* cart_abandon_rate â€” Proportion of carts abandoned (0â€“1).\n",
        "\n",
        "* churned â€” Binary label: 1 = churned, 0 = active.\n",
        "\n",
        "* feedback_text â€” Customer review or free-text feedback.\n",
        "\n",
        "* last_purchase_date â€” Date of last purchase \" (Kacem).\n",
        "\n",
        "\n"
      ],
      "metadata": {
        "id": "9Go0ETu0Nf9k"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "# ðŸ“Š SECTION B: Data Cleaning, Data  Discoveries\n",
        "\n",
        "Before jumping into A/B testing, machine learning, or anything data science related, we have some prerequisites to take care of:\n",
        "\n",
        "1. Data Cleaning\n",
        "\n",
        "* We need to make sure the data has no NA values. If so, we need to find out why the data has NA values and then replace the NA values.\n",
        "\n",
        "* We also want the data set to be in a readable format.\n",
        "\n",
        "2. Data Discoveries\n",
        "\n",
        "* Is there something interesting we can see based on aggregates, filtering, or summarizing the data?\n",
        "\n",
        "* For example, maybe we can group by the control group and the exposed group, and see what group had the most \"yes\".\n",
        "\n",
        "\n",
        "\n"
      ],
      "metadata": {
        "id": "AiV0nYDfUok_"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "## 1. Data Cleaning\n",
        "\n",
        "Lets first check if there are any NA values.\n"
      ],
      "metadata": {
        "id": "Iw0zPWHvUon6"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "### Checking for NA values:"
      ],
      "metadata": {
        "id": "wj43okcjWV5Q"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "\n",
        "# 1. do .isna() -> False is no NA, True if NA\n",
        "# 2. do .sum() -> sum up all 0's and 1's\n",
        "\n",
        "AB_df.isna().sum()\n",
        "\n"
      ],
      "metadata": {
        "id": "AmTpGfIQk8i0"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "Well there are no NA values. We do not have to worry about replacing NA values or understanding why they existed in the first place.\n",
        "\n",
        "We need to keep in mind that in real life data sets, they will almost always be messy."
      ],
      "metadata": {
        "id": "ajgH70BskgR_"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "### Checking for duplicates:\n",
        "\n",
        "Does our data frame have any duplicates?\n"
      ],
      "metadata": {
        "id": "Q0FQw51iWU2u"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# first calculate the shape\n",
        "AB_df.shape\n"
      ],
      "metadata": {
        "id": "RWOZwK6LWcfU"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# first calculate\n",
        "AB_df.drop_duplicates().shape\n",
        "\n",
        "\n"
      ],
      "metadata": {
        "id": "dq9mQ1wPWx1J"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "Since the size is the same for both the original data frame and the dropped duplicates data frame, we do not have any duplicates either!"
      ],
      "metadata": {
        "id": "8NGAVKdtXB8q"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "### Checking each variable's value:\n",
        "\n",
        "Do we have any inconsistent values?\n",
        "\n",
        "Lets just check the discrete or categorical values.\n"
      ],
      "metadata": {
        "id": "aJqWESk9kJA8"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# age:\n",
        "print(\"Age values:\", np.sort(AB_df[\"age\"].unique()))\n",
        "\n",
        "print()\n",
        "\n",
        "# gender:\n",
        "print(\"Gender values:\", AB_df[\"gender\"].unique())\n",
        "\n",
        "print()\n",
        "\n",
        "# country:\n",
        "print(\"Country values:\", AB_df[\"country\"].unique())\n"
      ],
      "metadata": {
        "id": "EudEkMG2kU32"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "I do not see any inconsitent or alarming values from the discrete or categorical variables.\n",
        "\n",
        "\n",
        "\n",
        "---"
      ],
      "metadata": {
        "id": "K_vcfRuNlmbQ"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "## 2. Data Discoveries\n",
        "\n",
        "\n",
        "\n",
        "\n",
        "\n",
        "Can we find any interesting discoveries in the Advertisement data set?\n",
        "\n",
        "As a follow up, can we visualize the interesting discoveries?\n"
      ],
      "metadata": {
        "id": "tPGg2srckgWv"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# recall that the AB df is:\n",
        "\n",
        "AB_df\n"
      ],
      "metadata": {
        "id": "SsNu1NLglojK"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "### What are the average income for male and females?\n",
        "\n",
        "Lets use groupby() in pandas to explore this!"
      ],
      "metadata": {
        "id": "6lhMRhpxaIRG"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "male_female_df = AB_df.groupby([\"gender\"])[\"annual_income\"].mean()\n",
        "\n",
        "male_female_df\n"
      ],
      "metadata": {
        "id": "3JLKceTmqBuO"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "print(\"Difference of female and male income:\", round(male_female_df[0] - male_female_df[1], 2))\n",
        "\n"
      ],
      "metadata": {
        "id": "XAQSoirDa4Wz"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "It appears that females have a higher income then men. The difference is about 728.32 dollars."
      ],
      "metadata": {
        "id": "NIhA6cJgayK_"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "### What are the average of continous variables for male and females?\n",
        "\n",
        "Now lets check all continous variables and the averages between male and females.\n"
      ],
      "metadata": {
        "id": "cTsDqMwLbLoR"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "male_female_df = AB_df.groupby([\"gender\"]).mean(numeric_only = True)\n",
        "\n",
        "male_female_df\n",
        "\n",
        "\n"
      ],
      "metadata": {
        "id": "dLQxfPaJpNPU"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "We do need to be careful. The average customer_id does not seem to tell us any useful information since customer_id is simply the id of a person regardless of their gender.\n",
        "\n",
        "Other variable averages between male and female honestly seem similar.\n",
        "\n",
        "Lets make a pie chart of the the number of males and females."
      ],
      "metadata": {
        "id": "z_uqRbPobaeQ"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "values = AB_df[\"gender\"].value_counts()\n",
        "\n",
        "values"
      ],
      "metadata": {
        "id": "sXDNGGCPv6lX"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "fig = plt.figure(figsize = (15,9))\n",
        "\n",
        "categories = [\"male\", \"female\"]\n",
        "color_list = [\"red\",\"blue\"]\n",
        "\n",
        "\n",
        "plt.pie(values,\n",
        "        labels = categories, # labels should be our categories\n",
        "        autopct = \"%1.1f%%\", # need this format for percent numbers\n",
        "        explode = [0, 0.1], # split the female part\n",
        "        shadow = True, # put shadow\n",
        "        startangle = 90)\n",
        "\n",
        "plt.title(\"Comparing proportions of Males and Females\", fontsize = 25);\n"
      ],
      "metadata": {
        "id": "eR7JOwMcwMES"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "Interesting. It appears the males and females have similar proportions, half and half."
      ],
      "metadata": {
        "id": "u0XdS-lrUoqY"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "### Average income by Country?\n",
        "\n",
        "Each country is bound to have different average income since each country has different GDP and economies.\n",
        "\n",
        "Lets see what the data suggests:"
      ],
      "metadata": {
        "id": "o4n9iVyekmSK"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "country_df = AB_df.groupby([\"country\"])[\"annual_income\"].mean()\n",
        "\n",
        "country_df"
      ],
      "metadata": {
        "id": "4nZwIdgce3N3"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "It is interesting to see that the average annual income values are in 50k or 100k values."
      ],
      "metadata": {
        "id": "Z5nX-h5kfJqD"
      }
    },
    {
      "cell_type": "markdown",
      "source": [],
      "metadata": {
        "id": "q3ICML9RfJr6"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "### Average continuous variables by Country?\n"
      ],
      "metadata": {
        "id": "MQPdWDujUos5"
      }
    },
    {
      "cell_type": "markdown",
      "source": [],
      "metadata": {
        "id": "L3h-QvD7fXg7"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "country_df = AB_df.groupby([\"country\"]).mean(numeric_only = True)\n",
        "\n",
        "country_df"
      ],
      "metadata": {
        "id": "0yUMpwzsfZDx"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "There seems to be more variance in annual_income, spending_score, num_purchases, avg purchase_value, and churned values compared to the male and female data set!\n",
        "\n"
      ],
      "metadata": {
        "id": "tqt-D2wBfHL2"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "Now lets make a pie chart to see the proportions of customers based on countries!"
      ],
      "metadata": {
        "id": "cf6FbLyPgjCB"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "country_values = AB_df[\"country\"].value_counts()\n",
        "\n",
        "country_values"
      ],
      "metadata": {
        "id": "KkAibJIUgrBw"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# make a list for our labels!\n",
        "\n",
        "# our categories!\n",
        "country_names = list(country_values.index)\n",
        "print(country_names)\n",
        "\n",
        "print(len(country_names))"
      ],
      "metadata": {
        "id": "1d4aPyeRhUFW"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "fig = plt.figure(figsize = (15,12))\n",
        "\n",
        "\n",
        "color_list = [\"red\",\"blue\",\"green\",\"yellow\",\"pink\",\"orange\",\n",
        "              \"gold\",\"lightblue\", \"purple\", \"lightgreen\"]\n",
        "\n",
        "\n",
        "plt.pie(country_values,\n",
        "        labels = country_names,\n",
        "        colors = color_list,\n",
        "        autopct = \"%1.1f%%\",\n",
        "        explode = [0.2,0,0,0,0,0,0,0,0,0],\n",
        "        shadow = True,\n",
        "\n",
        "        )\n",
        "\n",
        "plt.title(\"Comparing customer proportions of 10 different countries\", fontsize = 25);\n"
      ],
      "metadata": {
        "id": "20nrRdCVgZBn"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "GDP stands for Gross Domestic Product. A GDP record website states that \"GDP, or Gross Domestic Product, is the total monetary value of all goods and services produced and sold within a country during a specific time period, typically one year\" (\"GDP by Country\").\n",
        "\n",
        "\n",
        "According to the same source, it shows that the United States is the country with the biggest GDP.\n",
        "\n",
        "Therefore, it makes sense that the United States has the highest proportion of customers in this data set.\n"
      ],
      "metadata": {
        "id": "0d6H98v2fHOQ"
      }
    },
    {
      "cell_type": "markdown",
      "source": [],
      "metadata": {
        "id": "yqzEBmnVfHQt"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "# ðŸ”Ž SECTION C: Exploratory Data Analysis (EDA)\n",
        "\n",
        "According to a IBM article, it states that \"Exploratory data analysis is used by data scientists to analyze and investigate data sets and summarize their main characteristics, often employing data visualization methods\" (IBM).\n",
        "\n",
        "\n",
        "\n",
        "As stated above, a popular form of EDA is Data Visualization.\n",
        "\n",
        "* Is there any way we can visualize data so we can get a big picture of what we are dealing with?\n",
        "\n",
        "* Even a single histogram can give us information such as the distribution of the data.   \n",
        "\n"
      ],
      "metadata": {
        "id": "x98lmJ0ZUpDs"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "## Histograms of Continuous Variables\n",
        "\n",
        "Lets create histograms of multiple variables!\n"
      ],
      "metadata": {
        "id": "jXlwBF8xUpFB"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "AB_df"
      ],
      "metadata": {
        "id": "xYGRMV8rGG8u"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "Histogram of Annual Income:"
      ],
      "metadata": {
        "id": "oS0olhc6GE78"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "fig = plt.figure(figsize = (15,10))\n",
        "\n",
        "sns.histplot(AB_df, x = \"annual_income\", kde = True, color = \"green\")\n",
        "\n",
        "plt.title(\"Histogram of Annual Income for each customer\", fontsize = 25);\n",
        "\n"
      ],
      "metadata": {
        "id": "k5QsqCyFElVf"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "Histogram of Spending Score:"
      ],
      "metadata": {
        "id": "URHZnio2UpF8"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "fig = plt.figure(figsize = (15,10))\n",
        "\n",
        "sns.histplot(AB_df, x = \"spending_score\", color = \"red\")\n",
        "\n",
        "plt.title(\"Histogram of Spending Score for each customer\", fontsize = 25);\n",
        "\n"
      ],
      "metadata": {
        "id": "THNg5y4_GPth"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "Histogram of Number of Purchases:"
      ],
      "metadata": {
        "id": "092WFcTvUpHh"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "fig = plt.figure(figsize = (15,10))\n",
        "\n",
        "sns.histplot(AB_df, x = \"num_purchases\", color = \"blue\")\n",
        "\n",
        "plt.title(\"Histogram of number of purchases for each customer\", fontsize = 25);\n",
        "\n"
      ],
      "metadata": {
        "id": "nInGlft6GTZ5"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "Histogram of Average Purchases:"
      ],
      "metadata": {
        "id": "L6CPM7HLGOqY"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "fig = plt.figure(figsize = (15,10))\n",
        "\n",
        "sns.histplot(AB_df, x = \"avg_purchase_value\", kde = True, color = \"purple\")\n",
        "\n",
        "plt.title(\"Histogram of Average Purchase values for each customer\", fontsize = 25);\n",
        "\n"
      ],
      "metadata": {
        "id": "78F00Zd_GXhI"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "Histogram of Membership Years:"
      ],
      "metadata": {
        "id": "36KfVEjsGOsp"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "fig = plt.figure(figsize = (15,10))\n",
        "\n",
        "sns.histplot(AB_df, x = \"membership_years\", color = \"gold\")\n",
        "\n",
        "plt.title(\"Histogram of membership years for each customer\", fontsize = 25);\n",
        "\n"
      ],
      "metadata": {
        "id": "-BAlrF1FGdb4"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "Histogram of Website visits per month:"
      ],
      "metadata": {
        "id": "bVu8OUaBGOv8"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "fig = plt.figure(figsize = (15,10))\n",
        "\n",
        "sns.histplot(AB_df, x = \"website_visits_per_month\", color = \"gray\")\n",
        "\n",
        "plt.title(\"Histogram of website visits per month for each customer\", fontsize = 25);\n",
        "\n"
      ],
      "metadata": {
        "id": "KKkMKpgGGkWh"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "Histogram of Cart Abandon Rate:"
      ],
      "metadata": {
        "id": "T5R04OXcGOyq"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "fig = plt.figure(figsize = (15,10))\n",
        "\n",
        "sns.histplot(AB_df, x = \"cart_abandon_rate\", color = \"teal\")\n",
        "\n",
        "plt.title(\"Histogram of cart abandon rates for each customer\", fontsize = 25);\n",
        "\n"
      ],
      "metadata": {
        "id": "E36FumR2GoaJ"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "Histogram of Churn:\n"
      ],
      "metadata": {
        "id": "fTqLZIH6GPAv"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "fig = plt.figure(figsize = (15,10))\n",
        "\n",
        "sns.histplot(AB_df, x = \"churned\", color = \"black\")\n",
        "\n",
        "plt.title(\"Histogram on wheter each customer churned or not\", fontsize = 25);\n",
        "\n"
      ],
      "metadata": {
        "id": "9phooidYGs_2"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "# ðŸ“ SECTION D: Hypothesis Tests, p-values, Confidence Intervals, and Rejection Regions\n",
        "\n",
        "To understand how A/B testing works, we need to know some statistics defintions. Lets start off with statistical inference terms.\n",
        "\n",
        "\n",
        "## Statistical Inference terms:\n",
        "\n",
        "Statistical Inference is when we infer a population paramater based on a sample statistic. Here are some defintions to start us off.\n",
        "\n",
        "* Population: The full group of interest.\n",
        "\n",
        "EX: The population of people living in the United States\n",
        "\n",
        "* Sample: Only a part of the full group of interest.\n",
        "\n",
        "EX: Randomly pick 500 people in the United States.\n",
        "\n",
        "* Parameter: A unknown but fixed value that describes a population.\n",
        "\n",
        "EX: The population mean, $\\mu$.\n",
        "\n",
        "* Statistic: A known value based on the function of the data.\n",
        "\n",
        "EX: The sample mean, $\\bar x$.\n",
        "\n",
        "OR maybe the sample median, $x_{0.5}$\n",
        "\n",
        "Usually, statistics can be used as estimators. Estimators try to get a estimated value of the parameter. We put hats to denote estimators. For instance, a good estimator for the population mean $\\mu$ is the sample mean $\\bar x$.\n",
        "\n",
        "$\\widehat \\mu = \\bar x$\n",
        "\n",
        "\n",
        "---\n"
      ],
      "metadata": {
        "id": "6LLwJumgUwcB"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "## Random Variables (RV) Case Study\n",
        "\n",
        "Before we go on to residual assumptions lets review what a random variable is.\n",
        "\n",
        "The definition of a random variable (RV): A variable that has a random value. The value is not a fixed constant.\n",
        "\n",
        "* This is a example of a variable that is NOT a RV:\n",
        "\n",
        "$$x + 2 = 5$$\n",
        "\n",
        "In this case, $x = 3$ always. Notice $x$ is lower case here too. Lower case letters represent realized values.\n",
        "\n",
        "* Example of a RV:\n",
        "\n",
        "$$X \\sim Unif(0,1)$$\n",
        "\n",
        "Here, X can be any decimal value from 0 to 1 since it comes from a uniform distribution from 0 to 1. Notice that $X$ is capital here, we usually denote capital letters as a RV.\n",
        "\n",
        "RVs apply in the real world as well! Lets say that someone tries to measure a random person's height in the United States. That person would not always be 65 inches. We could have a person with a height of 62.7 inches, 74.91 inches, and so on.\n",
        "\n",
        "Lets pretend the people's heights are normally distributed with a population mean of 67 inches, and a population variance of 10 inches. A normal distribution is a reasonable assumption since people's heights are continous and can have decimal values, and a LOT of people live in the United States! The variable of a selected person's height is random and could be anything from this normal distribution!\n",
        "\n",
        "The notation is:\n",
        "\n",
        "$$X_i \\sim N(\\mu = 67, \\sigma^2 = 10)$$\n",
        "\n",
        "Where $X_i$ is the RV for the i-th person!\n",
        "\n",
        "Therefore, in $e_i \\sim N(0,\\sigma^2)$, the residuals ($e_i$) are RVs since they can have a random value from the Normal distribution.\n",
        "\n",
        "(This explenation originally appeared in my  Pokemon Mega Project.)\n",
        "\n",
        "---\n",
        "\n"
      ],
      "metadata": {
        "id": "h0U9VIg-PhCM"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "## Hypothesis Test\n",
        "\n",
        "Imagine a court room. You see a defendant being tried for robbing a bank. But we want to see all the evidence and then conclude if he actually robbed the bank or not.\n",
        "\n",
        "In the United States, courtrooms stand by the idea of: \"Innocent until PROVEN Guilty.\"\n",
        "\n",
        "\n",
        "During hypothesis testing, we have a Null Hypothesis and a Alternative Hypothesis.\n",
        "\n",
        "* Null Hypothesis,  $H_0$:\n",
        "\n",
        "The null hypothesis is generally the assumed value.\n",
        "\n",
        "Back in the court room, we first assume that the defendant is innocent.\n",
        "\n",
        "* Alternate Hypothesis, $H_a$ (or $H_1$):\n",
        "\n",
        "The alternate hypothesis is the value that the analyst tries prove is true.\n",
        "\n",
        "Back in the court room, the prosecutor wants to prove that the defendant is guilty. But his/her team needs evidence to prove this.\n",
        "\n",
        "\n",
        "* If we do not have enough evidence, then we \"fail to reject the Null Hypothesis\". (aka \"FTR\"- fail to reject)\n",
        "\n",
        "* If we do have enough evidence, then we \"reject the Null Hypothesis\".\n",
        "\n",
        "\n",
        "* We do NEVER say \"we accept the Null Hypothesis\". It is possible that this Null Hypothesis is wrong since we almost all cases, we do NOT know the parameter value!  \n",
        "\n",
        "* We do NEVER say \"we accept the Alternate Hypothesis\". In the Statistics world, we  can NEVER prove anything with 100% certanity!  \n",
        "\n",
        "\n",
        "(I will explain how we will get this evidence later on.)\n",
        "\n",
        "Now that you understand the courtroom example, lets jump into a sports example.\n",
        "\n",
        "\n",
        "### Hypothesis Testing in action:\n",
        "\n",
        "\n",
        "\n",
        "\n",
        "\n",
        "\n",
        "\n",
        "\n",
        "\n",
        "Lets pretend that we are doing a hypothesis test on Lebron James.\n",
        "\n",
        "Lets assume that a average NBA player makes about 10 points per game.\n",
        "\n",
        "We want to test if Lebron's average points per game is 10 points per game or NOT equal to 10 points per game.\n",
        "\n",
        "We will test a single mean.\n",
        "\n",
        "\n",
        "$H_0: \\mu = 10, \\text{Lebron averages 10 points per game}$\n",
        "\n",
        "$H_a: \\mu \\ne 10, \\text{Lebron does NOT average 10 points per game}$\n",
        "\n",
        "\n",
        "WARNING: Notice that we are using parameters in our hypothesis test. This is because statistical inference is about learning more about parameters ($\\mu$), NOT statistics ($\\hat \\mu$)!\n",
        "\n",
        "Anyone can observe statistics such as the sample mean $\\bar x$. But parameters are unknown. So why put a statistic such as $H_0: \\bar x = 10$? We may have already found that $\\bar x = 25.73$, so doing hypothesis tests on statistics is POINTLESS!\n",
        "\n",
        "Now any basketball fan knows  that Lebron James averages more than 10 points per game. But we need evidence to support our thinking.\n",
        "\n",
        "We can show statistical evidence of this in 3 ways:\n",
        "\n",
        "1. p-value\n",
        "\n",
        "\n",
        "2. Confidence Intervals\n",
        "\n",
        "\n",
        "3. Rejection Regions\n",
        "\n",
        "\n",
        "Lets explore these 3 aspects of statistical inference.  \n",
        "\n",
        "---\n",
        "\n",
        "\n",
        "\n"
      ],
      "metadata": {
        "id": "FAWLzetYUwd3"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "### Hypothesis test on single mean, customer example:\n",
        "\n",
        "According to the Federal Reserve Bank of St Louis, it states that: \"the mean personal income in the United States is 67,080 as of 2024\" (FRED).\n",
        "\n",
        "\n",
        "Now the Federal Reserve Bank of St Louis is part of the Federal Reserve System. Therefore they have economic data of the whole United States.\n",
        "\n",
        "Lets make a hypothesis test using this information.\n",
        "\n",
        "â— BE CAREFUL! We want to ONLY use the data from USA! Using different countries defeats the purpose of this hypothesis test since we ONLY know the personal average income in the USA!\n",
        "\n",
        "* The hypothesis test:\n",
        "\n",
        "$$H_0: \\mu_{USA} = 67,080$$\n",
        "\n",
        "$$H_a: \\mu_{USA} \\ne 67,080$$\n",
        "\n"
      ],
      "metadata": {
        "id": "2da7g8aoNnZw"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# filter the data to include only the USA\n",
        "# next, select just the annual income\n",
        "usa_income = AB_df[AB_df[\"country\"] == \"USA\"][\"annual_income\"]\n",
        "\n",
        "usa_income"
      ],
      "metadata": {
        "id": "7NdHjhx-q5nH"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# sample mean:\n",
        "xbar = usa_income.mean()\n",
        "\n",
        "# type cast to get rid of np.float64() -> also round by 2\n",
        "xbar = round(float(xbar),2)\n",
        "\n",
        "print(\"The sample mean is:\",xbar)\n"
      ],
      "metadata": {
        "id": "niJcs_6lq6gV"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "### What is happening and why is the sample mean way different than the null mean?\n",
        "\n",
        "Interesting. I think we are already going to reject the null hypothesis since $\\bar x_{USA} = 103614.36$ is way greater than $67080$.\n",
        "\n",
        "Perhaps this data set has selection bias. Maybe only rich people tend to buy products in this data set.\n",
        "\n",
        "Hypothesis tests really depend on randomly sampling, and I do not think random sampling of the United States happened here.\n",
        "\n",
        "Even if we reject the null hypothesis, that does not mean that the USA average annual income is NOT 67080. It most likely means that our data in this data frame is not randomly sampled."
      ],
      "metadata": {
        "id": "b6ciVV2oUoYi"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "## p-value\n",
        "\n",
        "To understand what a p-value does, we need to go over a few defintions first.\n",
        "\n",
        "\n",
        "\n",
        "\n",
        "\n",
        "**Test Statistic:**\n",
        "\n",
        "A numerical value that is calculated from the sample data. The value can help us determine if we reject the null hypothesis or not.\n",
        "\n",
        "* As a note, we almost always use a t-test (t-distribution)instead of Z-test (Normal-distributon).\n",
        "\n",
        "This is because we do almost never know the population variance and the normal distribution requires it.\n",
        "\n",
        "We can always calculate the sample variance using the data, and thats what the t-distribution requires. So lets use the t-distribution for this project!\n",
        "\n",
        "\n",
        "Here is a example of a one sample t-test statistic:\n",
        "\n",
        "$$t_{obs} = \\frac{\\bar x - \\mu_0}{\\frac{s}{\\sqrt{n}}}$$\n",
        "\n",
        "Where:\n",
        "\n",
        "* $t_{obs}$ is the observed test statistic.\n",
        "\n",
        "* $\\bar x$ is the sample mean. $\\bar x = \\frac{1}{n}\\sum^n_{i=1} x_i $\n",
        "\n",
        "* $\\mu_0$ is the null hypothesis population mean value.\n",
        "\n",
        "* $s$ is the sample standard deviation.\n",
        "\n",
        "* $n$ is the number of observations.\n",
        "\n",
        "\n",
        "**p-value:**\n",
        "\n",
        "The probablity of obtaining data (values after the test statistic) as extreme or more extreme than what was observed (test statistic), assuming the null hypothesis is true.\n",
        "\n",
        "There are multiple components of p-value:\n",
        "\n",
        "* p-value is a probablity value. The only possible numbers of a p-value is from [0,1].  \n",
        "\n",
        "* p-value is calculated assuming the null hypothesis is true.\n",
        "\n",
        "* The p-value comes from the test statistic and it's corresponding CDF (Cumulative Distribution Function). For instance, $t_obs$'s CDF is the t-distribution.\n",
        "\n",
        "* The lower p-value, the better, Lower p-values shows that the evidence is in favor of the alternate hypothesis.\n",
        "\n",
        "* Think about this, a low p-value shows someone that the probablity of obtaining this data or more extreme is unlikely IF we assume the null hypothesis is true.\n",
        "\n",
        "\n",
        "\n",
        "\n",
        "**Alpha Level/Significance Level:**\n",
        "\n",
        "This is denoted by $\\alpha$.\n",
        "\n",
        "This number is selected by the data scientist.\n",
        "\n",
        "Normally, people pick $\\alpha = 0.05$. But we need to use discretion. We may want to select a smaller alpha level such as $\\alpha = 0.01$ to be certain that we can reject the Null Hypothesis.\n",
        "\n",
        "\n",
        "\n",
        "### Interpretting p-values and alpha levels\n",
        "\n",
        "* Reject the null hypothesis:\n",
        "\n",
        "If the p-value is less than the alpha level, then we have statisical evidence to reject the null hypothesis!\n",
        "\n",
        "$$\\text{p-value} < \\alpha$$\n",
        "\n",
        "\n",
        "* Fail to reject the null hypothesis:\n",
        "\n",
        "If the p-value is greater than or equal to  the alpha level, then we do NOT have enough statisical evidence to reject the null hypothesis! We fail to reject the null.\n",
        "\n",
        "$$\\text{p-value} \\ge \\alpha$$\n",
        "\n",
        "\n",
        "\n"
      ],
      "metadata": {
        "id": "25Us_n43PI_z"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "### Lebron James example Part 1:\n",
        "\n",
        "Lets say the p-value is 0.00128, and the alpha level is $\\alpha = 0.05$.\n",
        "\n",
        "Since\n",
        "$\\text{p-value= .00128} < \\alpha = 0.05$, we have evidence to reject the null hypothesis! We have evidence that Lebron does NOT average 10 points per game\n",
        "\n",
        "---"
      ],
      "metadata": {
        "id": "X3jhkLxzjXHz"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "**Type 1 Error:**\n",
        "\n",
        "Type 1 Error is the probability that we reject a TRUE null hypothesis.\n",
        "\n",
        "Reducing the alpha level also reduces the possibility of Type 1 Error.\n",
        "\n",
        "\n",
        "\n",
        "In general, $\\text{Type 1 Error} = \\alpha$\n",
        "\n",
        "**Type 2 Error:**\n",
        "\n",
        "Type 2 Error is the probability that we FAIL to  reject a FALSE  null hypothesis.\n",
        "\n",
        "Notice that the Type 2 Error is the OPPOSITE of Type 1 Error! So if you know what Type 1 Error is, then you know what Type 2 Error is by imagining the opposite scenario!\n",
        "\n",
        "In general, Type 2 Error is calculated by: $\\text{Type 2 Error} = \\beta$\n",
        "\n",
        "\n",
        "\n",
        "**Power:**\n",
        "\n",
        "Power is just the ability to detect a false null hypothesis. This is the opposite of Type 2 Error.\n",
        "\n",
        "Since Power is the opposite of Type 2 Error, it is calculated by: $\\text{Power} = 1 - \\beta$\n",
        "\n",
        "We get more Power if:\n",
        "\n",
        "1. We have large $n$. More data means less variability. Our estimates become more credible.\n",
        "\n",
        "2. We have less variance $\\sigma^2$.\n",
        "\n",
        "3. We increase the alpha level. But be careful, this is not ethical. Increasing the alpha level means the Type 1 Error increases as well since $\\text{Type 1 Error} = \\alpha$\n",
        "\n",
        "4. If the real data has a signficantly different value compared to the null hypothesis, then Power increases.\n",
        "\n",
        "But we can not just pick a small null hypothesis and say we got more power. We want to pick a null hypothesis that we think is close to the real value.\n",
        "\n",
        "\n"
      ],
      "metadata": {
        "id": "sxMqpUXPKcC0"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "---\n",
        "\n",
        "### p-value on single mean, customer example:\n",
        "\n",
        "Lets set the alpha level as: $\\alpha = 0.01$\n"
      ],
      "metadata": {
        "id": "h3iT7sc59mRe"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "xbar = usa_income.mean()\n",
        "print(\"The sample mean is:\", round(xbar,3))\n",
        "\n",
        "mu_0 = 67080 # our null hypothesis population mean\n",
        "\n",
        "\n",
        "s = np.std(usa_income)\n",
        "print(\"The sample standard deviation is:\", round(s,3))\n",
        "\n",
        "\n",
        "n = len(usa_income)\n",
        "df = n - 1\n",
        "print(\"The number of observations is:\", n)\n",
        "print(\"The degrees of freedom is:\", df)\n",
        "\n",
        "\n",
        "t_obs = (xbar - mu_0) / (s / np.sqrt(n))\n",
        "\n",
        "print(\"The observed t-test statistic is:\", round(t_obs,3))\n",
        "\n",
        "\n",
        "\n"
      ],
      "metadata": {
        "id": "p4Vvheh69laW"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "Now we can get our p-value. This is for a two sided hypothesis test:"
      ],
      "metadata": {
        "id": "qlnhkGNaJUSN"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# Getting our p-value.\n",
        "p_value = 2 * (1 - stats.t.cdf(abs(t_obs), df))\n",
        "\n",
        "print(\"The p-value is:\", p_value)"
      ],
      "metadata": {
        "id": "eNuvhFSgJZJH"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "Since the p-value is essentially 0, then $p-value = 0.0 < \\alpha = 0.01$.\n",
        "\n",
        "We have enough evidence to reject the null hypothesis that the population mean is 67080 for the annual income of the USA. (But we know sampling bias could have occured, so we will examine this conclusion more in depth later.)\n",
        "\n",
        "So that was the hand made code method to find the p-value. What about getting the p-value in one line of code?\n"
      ],
      "metadata": {
        "id": "KMk2A1S2KBSa"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "p_value = stats.ttest_1samp(usa_income, popmean = mu_0).pvalue\n",
        "print(\"The p-value is:\", p_value)\n",
        "\n",
        "\n"
      ],
      "metadata": {
        "id": "vmuVKCkHKARW"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "The \"stats.ttest_1samp()\" code actually gives multiple values:"
      ],
      "metadata": {
        "id": "dQm3OdqGL3Px"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "stats.ttest_1samp(usa_income, popmean = mu_0)\n"
      ],
      "metadata": {
        "id": "xMTrYVTtL80y"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "## Confidence Interval\n",
        "\n",
        "A Confidence Interval (CI) is a interval that tries to capture where a parameter could be.  \n",
        "\n",
        "\n",
        "Here is a Confidence Interval for a t-distribution:\n",
        "\n",
        "$$\\bar x \\pm t_{df, (1- \\frac{\\alpha}{2})} * \\frac{s}{\\sqrt{n}}$$\n",
        "\n",
        "$$(\\bar x - t_{df, (1- \\frac{\\alpha}{2})} * \\frac{s}{\\sqrt{n}}, \\bar x + t_{df, (1- \\frac{\\alpha}{2})} * \\frac{s}{\\sqrt{n}} )$$\n",
        "\n",
        "Where:\n",
        "\n",
        "* $\\bar x$ is the sample mean.\n",
        "\n",
        "* $\\alpha$ is the alpha level.\n",
        "\n",
        "* $df$ is the degrees of freedom, where $df = n - 1$\n",
        "\n",
        "* $t_{df, (1- \\frac{\\alpha}{2})}$ is the critical value.\n",
        "\n",
        "* $s$ is the standard error.\n",
        "\n",
        "* $n$ is the number of observations.\n",
        "\n",
        "\n",
        "* The standard error is: $SE(\\bar x) = \\frac{s}{\\sqrt{n}}$\n",
        "\n",
        "* The margin of error is: $MOE = t_{df, (1- \\frac{\\alpha}{2})} * \\frac{s}{\\sqrt{n}}$\n",
        "\n",
        "\n",
        "So now we made a confidence interval. Now what?\n",
        "\n",
        "Well lets recall that we have alpha level. Depending on the alpha level, that is the percent confidence interval (CI) we have:\n",
        "\n",
        "$$(1-\\alpha)\\% \\text{ CI}$$\n",
        "\n",
        "If $\\alpha = 0.05, then we have a 95% CI.\n",
        "\n",
        "$$(1-\\alpha)\\% \\text{ CI} = (1-0.05)\\% \\text{ CI} = (0.95)\\% \\text{ CI}$$\n",
        "\n",
        "This means that we are 95% confident that our CI contains the population mean $\\mu$. (Or another parameter depending on our distribution.)\n",
        "\n",
        "The main idea of a CI is to find out what\n",
        "\n",
        "\n",
        "\n",
        "\n",
        "\n",
        "\n",
        "(Side note, we can NOT say that there is a 95% probability that the true mean value $\\mu$ lands in the 95% CI, because these are parameters and parameters have a fixed value. We can not speak probabilistically about them! We can only speak probabilistically of random variables (RV) such as $\\bar x$. Estimators and CIs are RV because they are functions of data, and data can give different numbers. CIs are basically just RV and some numbers, but that still counts as a RV. Parameters are unknown, but they they are fixed and constant numbers. )\n",
        "\n",
        "\n",
        "---\n",
        "\n",
        "\n",
        "\n",
        "\n"
      ],
      "metadata": {
        "id": "dPgdzvtTUwfx"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "### Lebron James example Part 2:\n",
        "\n",
        "Recall that $H_0: \\mu = 10, \\text{Lebron averages 10 points per game}$. Is this value in our CI?\n",
        "\n",
        "Lets say CI is $(24.094,31.836)$. $\\mu_0 = 10$ is DEFINTELY not contained in this CI!\n",
        "\n",
        "We have 95% confidence that this CI captures the true population $\\mu$. If we think that $\\mu = 27$, then that guess is plausible. But $\\mu = 10$ seems off target.\n",
        "\n",
        "\n",
        "\n",
        "\n",
        "\n",
        "\n",
        "\n",
        "Since $\\mu_0 = 10$ is not in our CI, we have evidence to reject the null hypothesis! We have evidence that Lebron does NOT average 10 points per game!\n",
        "\n",
        "---"
      ],
      "metadata": {
        "id": "N9uVsKt6jbqo"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "### Confidence Interval on single mean, customer example:\n"
      ],
      "metadata": {
        "id": "9WQExTGi9zqM"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# Recall that:\n",
        "\n",
        "xbar = usa_income.mean()\n",
        "print(\"The sample mean is:\", round(xbar,3))\n",
        "\n",
        "mu_0 = 67080 # our null hypothesis population mean\n",
        "\n",
        "\n",
        "s = np.std(usa_income)\n",
        "print(\"The sample standard deviation is:\", round(s,3))\n",
        "\n",
        "\n",
        "n = len(usa_income)\n",
        "df = n - 1\n",
        "print(\"The number of observations is:\", n)\n",
        "print(\"The degrees of freedom is:\", df)\n",
        "\n",
        "\n",
        "\n",
        "\n",
        "\n"
      ],
      "metadata": {
        "id": "ib1uLPkTrDgp"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# Lets make our CI using the information above, and some new code:\n",
        "\n",
        "# alpha level/ significance level:\n",
        "alpha = 0.01\n",
        "\n",
        "# ALERT: ppf stands for \"percent point function\" AKA the Inverse CDF!\n",
        "\n",
        "# the critical region:\n",
        "\n",
        "t_star = stats.t.ppf(1 - alpha/2, df)\n",
        "\n",
        "print(\"The critical region is:\", round(t_star,3))\n",
        "\n",
        "# Margin of error = critical region * standard error\n",
        "MOE =  t_star * (s / np.sqrt(n))\n",
        "\n",
        "print(\"Margin of Error value:\", round(MOE,3))\n",
        "\n",
        "# lower limit:\n",
        "LL = xbar - MOE\n",
        "LL = round(float(LL),3)\n",
        "\n",
        "\n",
        "# upper limit:\n",
        "UL = xbar + MOE\n",
        "UL = round(float(UL),3)\n",
        "\n",
        "# 99% CI:\n",
        "CI = (LL,UL) # type cast and round\n",
        "\n",
        "print(\"The 99% CI is:\", CI)\n"
      ],
      "metadata": {
        "id": "T0-DUSKArDm6"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "Since our null mean value of  $\\mu_{USA} = 67080$ is NOT contained inside the 99% CI of  $(101716.347, 105512.373)$, we have enough evidence to reject the null hypothesis! We are 99% confidence that this CI captures the population mean, and 67080 is not inside that CI!\n",
        "\n",
        "Is there a way to make CI with one line of code?\n",
        "\n",
        "YES! Lets do it right now!\n"
      ],
      "metadata": {
        "id": "Qf5KR59cPJ68"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# 1. (1-alpha)% CI\n",
        "# 2. df = n - 1\n",
        "# 3. loc = sample mean! NOT population mean or null mean!\n",
        "# 4. scale = SE(xbar) AKA stats.sem(x_vector)\n",
        "\n",
        "CI = stats.t.interval(1-alpha, df = df, loc = xbar, scale = stats.sem(usa_income))\n",
        "\n",
        "print(\"The 99% CI is:\", np.round(CI,3))\n",
        "\n"
      ],
      "metadata": {
        "id": "igZnG_87rDsF"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "The hand made CI and the the stats.t.interval CI appear to be similar!\n",
        "\n",
        "\n",
        "---"
      ],
      "metadata": {
        "id": "zqOv2Ry9RVcy"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "### One sided Confidence Intervals\n",
        "\n",
        "The above example was about a two sided CI.\n",
        "\n",
        "What if we wanted a one sided CI?\n",
        "\n",
        "Then the formulas would be different. The hypothesis tests would be different as well.\n",
        "\n",
        "* Let $A$ be a number value calculated by a CI.\n",
        "\n",
        "\n",
        "1. Upper one sided CI:\n",
        "\n",
        "* The Null Hypothesis is $H_0: \\mu \\le 10$\n",
        "\n",
        "* The Alternate Hypothesis is $H_a: \\mu > 10$ (We want $\\mu$ to NOT be in the CI to reject the Null Hypothesis)\n",
        "\n",
        "$$(-\\infty, A]$$\n",
        "\n",
        "Where:\n",
        "\n",
        "$$\\bar x -  t_{df, 0} = -\\infty$$\n",
        "\n",
        "And,\n",
        "\n",
        "$$\\bar x + t_{df, (1- \\alpha)} = A$$\n",
        "\n",
        "**WARNING:** Notice that we do NOT use $(1- \\frac{\\alpha}{2})$ for the critical region!!! That only applies for two sided CIs! For a one sided CI, we use $(1- \\alpha)$ instead!\n",
        "\n",
        "\n",
        "2. Lower one sided CI:\n",
        "\n",
        "\n",
        "* The Null Hypothesis is $H_0: \\mu \\ge 10$\n",
        "\n",
        "* The Alternate Hypothesis is $H_a: \\mu < 10$ (We want $\\mu$ to NOT be in the CI to reject the Null Hypothesis)\n",
        "\n",
        "$$[A, \\infty)$$\n",
        "\n",
        "\n",
        "Where:\n",
        "\n",
        "$$\\bar x - t_{df, (1- \\alpha)} = A$$\n",
        "\n",
        "And,\n",
        "\n",
        "$$\\bar x + t_{df, 0} = \\infty$$\n",
        "\n",
        "\n",
        "**WARNING:** Notice that we do NOT use $(1- \\frac{\\alpha}{2})$ for the critical region!!! That only applies for two sided CIs! For a one sided CI, we use $(1- \\alpha)$ instead!\n",
        "\n",
        "\n",
        "\n",
        "* NOTE: For this Project, there will be a main focus on two sided CIs and two sided hypothesis tests. Most A/B testing experiments use a two sided test because measuring any kind of difference is more standard and safer.\n",
        "\n",
        "---\n"
      ],
      "metadata": {
        "id": "mk4sVgsElAOa"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "## Rejection Region\n",
        "\n",
        "Rejection Region uses the critical regions of a distribution.\n",
        "\n",
        "For simplicity, lets use a normal distribution. For a two sided hypothesis test, the Z scores of the 95% percentiles are:\n",
        "\n",
        "$$z_{\\frac{0.05}{2}} = -1.96$$\n",
        "\n",
        "$$z_{1 - \\frac{0.05}{2}} = 1.96$$\n",
        "\n",
        "So if Z test statistic $z_{obs}$ is lower than $-1.96$ or higher than $1.96$, then we can reject the null hypothesis! We need the test statistic to be in the rejection region to reject the null hypothesis.\n",
        "\n",
        "Our rejection region in this scenario is:\n",
        "\n",
        "$$z_{obs} < -1.96 \\text{ OR } z_{obs} >1.96$$\n",
        "\n",
        "\n",
        "---\n",
        "\n",
        "\n"
      ],
      "metadata": {
        "id": "CntWt6NOPTdT"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "### Lebron James example Part 3:\n",
        "\n",
        "---\n",
        "\n",
        "\n",
        "\n",
        "Lets say the test statistic is $t_{obs} = 4.23$.\n",
        "\n",
        "Lets say that the rejection region is:\n",
        "\n",
        "$$t_{obs} < -2.12 \\text{ OR } t_{obs} > 2.12$$\n",
        "\n",
        "\n",
        "\n",
        "Since $t_{obs} = 4.23 > 2.12$, we have evidence to reject the null hypothesis! We have evidence that Lebron does NOT average 10 points per game\n",
        "\n",
        "---"
      ],
      "metadata": {
        "id": "gTdXAt8dPJCy"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "### Rejection Region on single mean, customer example:\n",
        "\n",
        "Scipy does not have a built in code for Rejection Regions, we need to hand make this ourselves!\n",
        "\n"
      ],
      "metadata": {
        "id": "rFBPMBLaPJFB"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "\n",
        "rejection_region = (round(float(stats.t.ppf(alpha/2, df)),3),\n",
        "                    round(float(stats.t.ppf(1 - alpha/2, df)),3))\n",
        "\n",
        "print(\"The rejection region is:\", rejection_region )\n",
        "print(\"Recall that the observed test statistic is:\", round(t_obs,3))\n"
      ],
      "metadata": {
        "id": "Sma0VlCLrJzx"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "Since the test statistic of 49.621 is OUTSIDE the rejection region, of (-2.578, 2.578), we can reject the null hypothesis!\n",
        "\n"
      ],
      "metadata": {
        "id": "fwg8r8YmWAJn"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "### So we rejected the null hypothesis for the customer example. Does that mean that US population does not have a average annual income of 67080?\n",
        "\n",
        "* NO! We can NOT be slaves to what the data or results tell us! We need to think analytically and outside of the box! We ALWAYS have to consider CONTEXT of the data!\n",
        "\n",
        "* I mentioned that there is a strong possibility that sampling bias that occured! Perhaps this data set only included mainly middle class and rich class customers!\n",
        "\n",
        "* For a sample to truly be representative of a population, we need a RANDOM sampling process! We can suspect that it is possible that randomly sampling did NOT happen here!\n",
        "\n",
        "\n",
        "**CONCLUSION:**\n",
        "\n",
        "In this example, our test statistics, p-values, and confidence intervals, are not credible. Either we need a hypothesis that makes sense or we need truly random sampled data!\n",
        "\n",
        "* We can NOT give a reliable market strategy in this case!\n",
        "\n",
        "---"
      ],
      "metadata": {
        "id": "vEvCHUaXTC8u"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "## So do we need to check all 3 methods of p-value, confidence intervals, and rejection regions?\n",
        "\n",
        "\n",
        "No. We just need to check the p-value and see if it is less than the alpha level to reject the null hypothesis, or fail to reject in the case the p-value is greater than the alpha level.\n",
        "\n",
        "\n",
        "Checking only the p-value is sufficient enough since all 3 methods will either result in rejecting the null hypothesis, or failing to reject the null.\n",
        "\n",
        "We will NOT have a scenario where the p-value will reject the null hypothesis, but the CI fails to reject the null hypothesis.\n",
        "\n",
        "From now on, simply using the p-value is good enough.\n",
        "\n",
        "---"
      ],
      "metadata": {
        "id": "3mfXJCo-WAL2"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "# ðŸ‘¨â€ðŸ‘¦ SECTION E: Diving into A/B Testing\n",
        "\n",
        "A/B testing is just two sample testing. Thats it!\n",
        "\n",
        "\n",
        "The main idea of A/B testing is if there is a difference from group A or group B. Many A/B tests deal with a control and a treatment group.\n",
        "\n",
        "Lets start off by defining what the hypothesis tests look like:"
      ],
      "metadata": {
        "id": "yw6SMsOAWAN_"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "## Two sample Mean tests\n",
        "\n",
        "When we want to find if two populations are different or not, we use the following hypothesis test:\n",
        "\n",
        "$$H_0: \\mu_A = \\mu_B,  \\text{(The population means are the same.) }$$\n",
        "\n",
        "$$H_a: \\mu_A \\ne \\mu_B,  \\text{(The population means are NOT the same.) }$$\n",
        "\n",
        "\n",
        "Alternatively, we can do:\n",
        "\n",
        "$$H_0: \\mu_A - \\mu_B = 0,  \\text{(The population means are the same.) }$$\n",
        "\n",
        "$$H_a: \\mu_A - \\mu_B \\ne 0,  \\text{(The population means are NOT the same.) }$$\n",
        "\n",
        "\n",
        "* So what are the formulas for the test statistic $t_{obs}$? It depends on the variance of both groups. Do the groups have similar variances or different variances?\n"
      ],
      "metadata": {
        "id": "aqwsP8GaWlwT"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "## Scenario 1: Equal Variances for both groups\n",
        "\n",
        "If we think that the variances for both groups are the same, then we use this test statistic formula:\n",
        "\n",
        "\n",
        "$$t_{obs} = \\frac{\\bar x_A - \\bar x_B}{s_p \\sqrt{\\frac{1}{n_A} + \\frac{1}{n_B}}}$$\n",
        "\n",
        "Where the pooled variance $s_p$ formula is:\n",
        "\n",
        "$$s_p = \\sqrt{\\frac{(n_A -1)* s^2_A + (n_B -1)* s^2_B}{n_1 + n_B - 2}}$$\n",
        "\n",
        "Degrees of freedom are calculate differently from a single mean test:\n",
        "\n",
        "$$df = n_A + n_B - 2$$\n",
        "\n",
        "Where:\n",
        "\n",
        "* $\\bar x_A$ and $\\bar x_B$ are the sample means for group A and group B respectively.\n",
        "\n",
        "* $n_A$ and $n_B$ are the sample size for group A and group B respectively.\n",
        "\n",
        "\n"
      ],
      "metadata": {
        "id": "394UFoR3WnVi"
      }
    },
    {
      "cell_type": "markdown",
      "source": [],
      "metadata": {
        "id": "9mz-eJCxghtg"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "## Scenario 2: Un-Equal Variances for both groups\n",
        "\n",
        "\n",
        "If we think that the variances for both groups are the NOT the same, then we use this test statistic formula:\n",
        "\n",
        "\n",
        "$$t_{obs} = \\frac{\\bar x_A - \\bar x_B}{\\sqrt{\\frac{s^2_A}{n_A} + \\frac{s^2_B}{n_B} } }$$\n",
        "\n",
        "\n",
        "The degrees of freedom is calculated with a big formula. We even have a special name for the degrees of freedom of unequal variances, the Satterthwaite degrees of freedom:\n",
        "\n",
        "$$df = \\frac{(\\frac{s^2_A}{n_A} + \\frac{s^2_B}{n_B})^2}{\\frac{(\\frac{s^2_A} {n_A})^2}{n_A-1} + \\frac{(\\frac{s^2_B} {n_B})^2}{n_B-1}}$$\n",
        "\n",
        "\n",
        "\n",
        "NOTE: This type of degrees of freedom can have decimal values!\n",
        "\n",
        "\n"
      ],
      "metadata": {
        "id": "UhCh2TNughvh"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "### How do we know if the population variances are equal or not?\n",
        "\n",
        "Well one way to determine this is by checking the sample variances for group A and group B. If they are similar, they can be assumed to have equal variances.\n",
        "\n",
        "If one sample variance is bigger by a factor of 3, then we assume unequal variances. Lets go over those formulas next.\n",
        "\n",
        "We can check the sample variance be doing \"series_name.var()\".\n"
      ],
      "metadata": {
        "id": "iJE9ER48ghxn"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "## APPLICATION: A/B Testing for two Means (Male and Female annual_income Means)\n",
        "\n"
      ],
      "metadata": {
        "id": "kPjS5O4kewmU"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "Recall in Section B we grouped the data by males and females. Here is a refresher:"
      ],
      "metadata": {
        "id": "yi_zEX3RdRiG"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "male_female_df = AB_df.groupby([\"gender\"]).mean(numeric_only = True)\n",
        "\n",
        "male_female_df\n",
        "\n",
        "\n",
        "\n"
      ],
      "metadata": {
        "id": "fx2pnxwih-Vo"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "All the variable mean values seem similar for both Female and Male. But annual_income seems to have somewhat different values.\n",
        "\n",
        "Lets do a A/B Test on two means for Female and Males annual_income!\n",
        "\n"
      ],
      "metadata": {
        "id": "7f8gyhfuno4r"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "### Hypothesis Test:\n",
        "\n",
        "Lets test the annual_income means of:\n",
        "\n",
        "$$H_0: \\mu_{Female} - \\mu_{Male} = 0$$\n",
        "\n",
        "$$H_a: \\mu_{Female} - \\mu_{Male} \\ne 0$$\n",
        "\n",
        "\n",
        "Lets first discover if the sample variances for both the Female and Male groups are similar!\n",
        "\n",
        "Lets also set our alpha level to be: $\\alpha = 0.05$\n"
      ],
      "metadata": {
        "id": "Xrr2pICjoOXS"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# alpha level checker\n",
        "\n",
        "def alpha_level_checker(p_value, alpha_level):\n",
        "  if p_value < alpha_level:\n",
        "    print(\"Since p_value < alpha_level, we have enough evidence to reject the null hypothesis!\")\n",
        "  else:\n",
        "    print(\"Since p_value >= alpha_level, we fail to reject the null hypothesis, we do not have enough evidence.\")\n",
        "\n",
        "# set alpha = 0.05\n",
        "\n",
        "alpha = 0.05\n"
      ],
      "metadata": {
        "id": "k3W8XLrEvGy6"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# first filter the data to get female and male groups:\n",
        "\n",
        "female_income = AB_df[AB_df[\"gender\"] == \"Female\"][\"annual_income\"]\n",
        "# print(\"Series of female income:\", female_income)\n",
        "\n",
        "male_income = AB_df[AB_df[\"gender\"] == \"Male\"][\"annual_income\"]\n",
        "# print(\"Series of male income:\", male_income)\n",
        "\n",
        "\n",
        "\n",
        "# second get sample means:\n",
        "\n",
        "# female sample mean:\n",
        "xbar_f = float(AB_df[AB_df[\"gender\"] == \"Female\"][\"annual_income\"].mean())\n",
        "print(\"Female Income sample Mean:\", round(xbar_f,3))\n",
        "\n",
        "# male sample mean:\n",
        "xbar_m = float(AB_df[AB_df[\"gender\"] == \"Male\"][\"annual_income\"].mean())\n",
        "print(\"Male Income sample Mean:\", round(xbar_m,3))\n",
        "\n",
        "\n",
        "print() # extra space\n",
        "\n",
        "# secondly filter the data to get the sample variances\n",
        "\n",
        "# female sample variance:\n",
        "s2_f = float(AB_df[AB_df[\"gender\"] == \"Female\"][\"annual_income\"].var())\n",
        "print(\"Female Income sample Variance:\", round(s2_f,3))\n",
        "\n",
        "# male sample variance:\n",
        "s2_m = float(AB_df[AB_df[\"gender\"] == \"Male\"][\"annual_income\"].var())\n",
        "print(\"Male Income sample Variance: \", round(s2_m,3))\n",
        "\n",
        "\n",
        "print()\n",
        "\n",
        "# Is one of the sample variances bigger than a factor of 3?\n",
        "\n",
        "print(\"Sample variance ratio:\", round(s2_f/s2_m,3) )\n",
        "\n"
      ],
      "metadata": {
        "id": "NJsfwLUWo9gu"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "Since the sample variance is 1.032 and not 3, we can assume equal variances.\n",
        "\n",
        "THEREFORE, lets use the equal variance  test statistic! (Check scenario 1)\n",
        "\n",
        "**NOTE:** Since I already did handmade coding for the single mean tests, I will focus on built in tests for future statistical tests.  \n"
      ],
      "metadata": {
        "id": "Q7QKW3Kqo9Am"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "### p-value:\n"
      ],
      "metadata": {
        "id": "UcmOpjlIwMfC"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "from scipy.stats import ttest_ind\n",
        "\n",
        "# 2 sample t-test code format:\n",
        "# ttest_ind(vector_A, vector_B, equal_var = True/False)\n",
        "\n",
        "# ALERT: do NOT do: ttest_ind(mean_A, mean_B, equal_var = True/False)\n",
        "# WE WANT DATA, NOT sample means!\n",
        "\n",
        "# t_obs -> the t-test statistic\n",
        "\n",
        "t_obs, p_value = ttest_ind(female_income, male_income, equal_var = True )\n",
        "\n",
        "print(\"The t-test statistic:\", round(t_obs,3))\n",
        "print(\"The p-value: \", round(p_value,3))\n",
        "\n",
        "\n",
        "print()\n",
        "\n",
        "alpha_level_checker(p_value, alpha)\n"
      ],
      "metadata": {
        "id": "UrgPftcZrvv0"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "### Conclusion and Potential Marketing Strategy:\n",
        "\n",
        "* Since $p-value = 0.35$ is greater than $\\alpha = 0.05$, we fail to reject the null hypothesis!\n",
        "\n",
        "* We conclude that we do not have enough evidence to say that average annual income levels for females and males are different!\n",
        "\n",
        "\n",
        "* Marketing Implication: As a possible marketing tactic, a company should NOT focus on targeting a specific gender to maximize profits. It does not seem like a company will get significantly more revenue if we simply target male customers.\n",
        "\n",
        "\n",
        "\n",
        "---\n",
        "\n",
        "We just did our first A/B Testing, huraahhh!\n",
        "\n",
        "But A/B Testing does not just use different means, lets examine a A/B Testing that uses different proportions."
      ],
      "metadata": {
        "id": "PHDV_4FsrMIY"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "## Two sample Proportion tests\n",
        "\n",
        "In the real life, A/B tests usually measure a metric such as wheter a person clicked a link or not.\n",
        "\n",
        "\n",
        "In this case, we would use sample proportions ($\\hat p$), not sample means ($\\bar x$).\n",
        "\n",
        "\n",
        "\n",
        "### Hypothesis test for population proportions\n",
        "\n",
        "We first assume the proportions are similar, we do:\n",
        "\n",
        "$$H_0: p_A = p_B, \\text{ The population proportions are the same}$$\n",
        "\n",
        "$$H_0: p_A \\ne p_B, \\text{ The population proportions are NOT the same}$$\n",
        "\n",
        "\n",
        "### Test statistic for two proportions\n",
        "\n",
        "* We first need to make pooled $\\hat p$.\n",
        "\n",
        "$\\widehat p_T = \\frac{x_A + x_B}{n_A + n_B}$\n",
        "\n",
        "\n",
        "* Next, we calculate our test statistic $Z_{obs}$:\n",
        "\n",
        "$$Z_{obs} = \\frac{\\widehat p_A - \\widehat p_B}{ \\sqrt{\\widehat p_T (1 - \\widehat p_T) (\\frac{1}{n_A} + \\frac{1}{n_B}) )  } }$$\n",
        "\n",
        "\n",
        "Where:\n",
        "\n",
        "* $x_A$ and $x_B$ are the number of group A and group B \"successes\" respectively.\n",
        "\n",
        "* $n_A$ and $n_B$ are the number of group A and group B observations respectively.\n",
        "\n",
        "\n",
        "* $\\widehat p_A = \\frac{x_A}{n_A}$ is the sample proportion of group A.\n",
        "\n",
        "\n",
        "* $\\widehat p_B = \\frac{x_B}{n_B}$ is the sample proportion of group B.\n",
        "\n",
        "\n",
        "\n",
        "### Why are we using a Z-test statistic instead of t-test statistic?\n",
        "\n",
        "We use a Z-test because the proportions can be approximately normally distributed. Recall that we use Z-tests when we use a normal distribution!\n",
        "\n",
        "\n",
        "Firstly, we need to satisfy the general conditions of:\n",
        "\n",
        "$n_A \\widehat p_A \\ge 5$\n",
        "\n",
        "$n_A (1-\\widehat p_A) \\ge 5$\n",
        "\n",
        "$n_B \\widehat p_B \\ge 5$\n",
        "\n",
        "$n_B (1-\\widehat p_B) \\ge 5$\n",
        "\n",
        "Using a proportions test will give a credible Z-score!\n",
        "\n",
        "Secondly, if these conditions are met, the sample proportion of succceses $\\hat p = \\frac{x}{n}$ is approximately Normal($p,\\frac{p(1-p)}{n}$).\n",
        "\n",
        "$$\\widehat p \\sim N(p,\\frac{p(1-p)}{n})$$\n",
        "\n",
        "\n"
      ],
      "metadata": {
        "id": "TBJ2AGTvdRm_"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "## APPLICATION: A/B Testing for two Proportions (USA vs non-USA country churn rates)\n",
        "\n"
      ],
      "metadata": {
        "id": "KOYzF_IoA4-r"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "What if we check the Churn of the customers from the USA vs any other country?\n",
        "\n",
        "Recall that Churned just means a customer stopped using a product or service."
      ],
      "metadata": {
        "id": "xQub4xwgWnWb"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# create churn series:\n",
        "usa_churn = AB_df[AB_df[\"country\"] == \"USA\"][\"churned\"]\n",
        "other_churn = AB_df[AB_df[\"country\"] != \"USA\"][\"churned\"]\n",
        "\n",
        "# usa churn sample proportion:\n",
        "p_hat_usa = usa_churn.mean()\n",
        "print(\"USA's churn proportion:\", round(p_hat_usa,3))\n",
        "\n",
        "# other churn sample proportion:\n",
        "p_hat_other = other_churn.mean()\n",
        "print(\"Other countries's churn proportion:\", round(p_hat_other,3))\n",
        "\n",
        "\n",
        "print() # extra space\n",
        "\n",
        "# lets find the number of observations for each group:\n",
        "\n",
        "n_usa = len(usa_churn)\n",
        "print(\"Number of customers in the USA:\", n_usa)\n",
        "\n",
        "n_other = len(other_churn)\n",
        "print(\"Number of customers in other countries:\", n_other)"
      ],
      "metadata": {
        "id": "KykD4x2ar9_b"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "Oof, it appears that the USA and other countries seem to have similar churn proportions. Lets make it official by doing a 2 proportions test."
      ],
      "metadata": {
        "id": "ewuVphM85VCO"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "### Hypothesis Test:\n",
        "\n",
        "Let us test:\n",
        "\n",
        "$$H_0: p_{USA} - p_{other} = 0$$\n",
        "\n",
        "$$H_a: p_{USA} - p_{other} \\ne 0$$\n",
        "\n",
        "\n",
        "**WARNING:** SciPy does NOT have the two proportions test. We will have to use another model called statsmodel.\n"
      ],
      "metadata": {
        "id": "4b1KngStr-Hj"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "### p-value:"
      ],
      "metadata": {
        "id": "O5FxjiD08zKj"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "from statsmodels.stats.proportion import proportions_ztest\n",
        "\n",
        "churn_list = [p_hat_usa, p_hat_other]\n",
        "\n",
        "n_list = [n_usa, n_other]\n",
        "\n",
        "z_obs, p_value = proportions_ztest(churn_list, n_list, alternative = \"two-sided\")\n",
        "\n",
        "\n",
        "print(\"The z test statistic is:\", round(z_obs,3))\n",
        "print(\"The p-value is:\", round(p_value,3))\n",
        "\n",
        "alpha_level_checker(p_value, alpha)\n"
      ],
      "metadata": {
        "id": "alvQfLUL6VoR"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "### Conclusion and Potential Marketing Strategy:\n",
        "\n",
        "* Since $p-value = 0.80$ is greater than $\\alpha = 0.05$, we fail to reject the null hypothesis!\n",
        "\n",
        "* We conclude that we do not have enough evidence to say that average churn proportions for the USA and other countries are different!\n",
        "\n",
        "* It is note worthy to mention that the p-value is 0.80, this is a extremely high value. This shows that there is litte difference between the groups.\n",
        "\n",
        "\n",
        "* Marketing Implication: As a possible marketing tactic, a company should NOT focus on targeting a non-USA countries to avoid high churn rates. Both the USA and non-USA customers seem to have similar churn rates, so the attention should be somewhere else.  \n",
        "\n",
        "\n",
        "\n",
        "\n",
        "---\n",
        "Lets do one more A/B test. But this A/B test will be a interesting one, we will be using word ratings from customers."
      ],
      "metadata": {
        "id": "oGz-fvJlr-LS"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "# ðŸŒ´ SECTION F: Combining Natural Language Processing (NLP) and A/B Testing\n",
        "\n",
        "According to a AWS website, it states that \"Natural language processing (NLP) is technology that allows computers to interpret, manipulate, and comprehend human language ... Natural language processing is key in analyzing this data for actionable business insights (\"NLP Explained - AWS\").\n",
        "\n",
        "\n",
        "\n",
        "\n",
        "\n",
        "* One application of NLP is sentiment analysis. This is simply a process where we try to find any positive or negative words in a text, and assign them values. Lets use this application of NLP!\n",
        "\n",
        "\n",
        "\n",
        "* What if we analyze which genders tend to be more satisified using customer text ratings?\n",
        "\n",
        "\n",
        "\n",
        "To do sentiment analysis, lets assign:\n",
        "\n",
        "* Lets give 1 point to any positive words such as: \"good\", \"fast\", \"amazing\"\n",
        "\n",
        "* Lets give a -1 point to any negative words such as: \"bad\", \"poor\", \"slow\"\n",
        "\n",
        "\n",
        "\n",
        "---"
      ],
      "metadata": {
        "id": "zyt_N_dTB0D_"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "### Editing and examing the text data:\n",
        "\n",
        "Lets see our text data:\n"
      ],
      "metadata": {
        "id": "TOWrISesNBzf"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "AB_df[\"feedback_text\"]\n",
        "\n",
        "\n"
      ],
      "metadata": {
        "id": "9eX6kgawSjKA"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "We will probably need to standardize the text so that it is all lower case. That way, we do not have to run into any complications that deal with capitalization cases!"
      ],
      "metadata": {
        "id": "8Y8f_-rRSxrO"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# assign the edited lower case text to the original text!\n",
        "AB_df[\"feedback_text\"] = AB_df[\"feedback_text\"].str.lower()\n",
        "\n",
        "AB_df[\"feedback_text\"]\n",
        "\n",
        "\n",
        "\n"
      ],
      "metadata": {
        "id": "O0ev3zT8S-P7"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "That seems to do the trick!\n",
        "\n",
        "Lets also remove punctuation marks such as periods, commas, question marks, and exclamation marks."
      ],
      "metadata": {
        "id": "GIgSSWmnTg67"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "import string # this is the library that deals with strings\n",
        "\n",
        "# ALERT:\n",
        "# str.maketrans(x, y, z)\n",
        "\n",
        "# x-> character to replace.\n",
        "# y-> character to that will replace x\n",
        "# z-> character that completely deletes\n",
        "\n",
        "# str.maketrans(\"\", \"\", string.punctuation)\n",
        "\n",
        "# nothing gets replaced\n",
        "# only punctiation marks get deleted\n",
        "\n",
        "\n",
        "# function:\n",
        "def clean_text(the_text):\n",
        "  # remove punctuation symbols\n",
        "  translator_variable = str.maketrans(\"\",\"\",string.punctuation)\n",
        "\n",
        "  # the_text is our parameter\n",
        "  # the_text.translate uses our translator_variable to make edits!\n",
        "\n",
        "  the_text = the_text.translate(translator_variable)\n",
        "\n",
        "  return the_text\n",
        "\n",
        "\n",
        "# clean the feedback_text series:\n",
        "# use .apply()\n",
        "\n",
        "clean_series = AB_df[\"feedback_text\"].apply(clean_text)\n",
        "\n",
        "\n",
        "# create a new variable\n",
        "\n",
        "AB_df[\"clean_feedback_text\"] = clean_series\n",
        "\n",
        "\n",
        "AB_df[\"clean_feedback_text\"]"
      ],
      "metadata": {
        "id": "nOtPcT7kUX-Z"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "Ok, we now have no punctuation marks.\n",
        "\n",
        "We need to do one more thing. We need to make a list of positive and negative words.\n",
        "\n",
        "We first need to find the unique words of the feedback_text!"
      ],
      "metadata": {
        "id": "7ityKRw7ThCI"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "\n",
        "\n",
        "AB_df[\"clean_feedback_text\"].unique()"
      ],
      "metadata": {
        "id": "PcKtK6VTeTFc"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# list of positive words:\n",
        "positive_words = [\"love\",\"fast\",\"satisfied\",\"good\",\"excellent\",\n",
        "                  \"great\",\"easy\"]\n",
        "\n",
        "# list of negative words:\n",
        "negative_words = [\"poor\",\"broke\",\"difficult\",\"slow\",\"not\",\n",
        "                  \"unhelpful\"]\n",
        "\n",
        "\n"
      ],
      "metadata": {
        "id": "3h2dhgE8XsfJ"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "We also have to ultimately to assign scores based on these positive and negative words.\n"
      ],
      "metadata": {
        "id": "UNIcCtpPThGU"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# make a function that sums up the sentiment score:\n",
        "\n",
        "def emotion_score(text):\n",
        "  split_words = text.split()\n",
        "  positive_score = sum(x in positive_words for x in split_words)\n",
        "  negative_score = sum(x in negative_words for x in split_words)\n",
        "  return positive_score - negative_score\n",
        "\n",
        "\n",
        "final_scores = clean_series.apply(emotion_score)\n",
        "\n",
        "AB_df[\"emotion_scores\"] = final_scores\n",
        "AB_df[\"emotion_scores\"]\n",
        "\n",
        "\n"
      ],
      "metadata": {
        "id": "25aLQ8h9eN6p"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "How many score values do we have?"
      ],
      "metadata": {
        "id": "ET_uAiGIhAqn"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# inner is inclusive, outer is EXCLUSIVE!!!\n",
        "\n",
        "for x in range(-5, 5 + 1):\n",
        "  print(\"We have this much \"+ str(x) + \" scores:\", sum(AB_df[\"emotion_scores\"] == x))\n",
        "\n"
      ],
      "metadata": {
        "id": "6pofve7deN_u"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "It appears that most customers were satisified with their purchases.\n",
        "\n",
        "\n"
      ],
      "metadata": {
        "id": "60RYT3P9eOIY"
      }
    },
    {
      "cell_type": "markdown",
      "source": [],
      "metadata": {
        "id": "I7iPfiPFeOM-"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "#sample_text = \"YOOOO its me ya boy!\"\n",
        "\n",
        "#print(sample_text)\n",
        "#print(sample_text.split())"
      ],
      "metadata": {
        "id": "rqjiJPoyiaaS"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [],
      "metadata": {
        "id": "u1e_j_qZeOSS"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "AB_df"
      ],
      "metadata": {
        "id": "6L7WAuJckn40"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "## APPLICATION: A/B Testing for two Means (Young and Elderly people on text ratings)\n",
        "\n",
        "Lets find out if young people and middle age to seniors (elder) people have different average emotions in their text ratings.\n",
        "\n",
        "* Let us consider young people as anyone from age 0 to 40.\n",
        "\n",
        "* Meanwhile, lets consider elder people as age 41 or older.  \n"
      ],
      "metadata": {
        "id": "fMoq9j9sTa8G"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "### Hypothesis Test:\n",
        "\n",
        "Lets do a hypothesis test on a difference of two means:\n",
        "\n",
        "Lets test the emotion_scores means of:\n",
        "\n",
        "$$H_0: \\mu_{Young} - \\mu_{Elder} = 0$$\n",
        "\n",
        "$$H_a: \\mu_{Young} - \\mu_{Elder} \\ne 0$$\n",
        "\n",
        "\n",
        "Lets first discover if the sample variances for both the Female and Male groups are similar!\n",
        "\n",
        "Lets also set our alpha level to be: $\\alpha = 0.05$\n",
        "\n"
      ],
      "metadata": {
        "id": "N6wATEi-B2wP"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# first filter the data to get female and male groups:\n",
        "\n",
        "young_emotion = AB_df[(AB_df[\"age\"] >= 0) & (AB_df[\"age\"] <= 40) ][\"emotion_scores\"]\n",
        "# print(\"Series of young emotion scores:\", young_emotion)\n",
        "\n",
        "elder_emotion = AB_df[AB_df[\"age\"] >= 41][\"emotion_scores\"]\n",
        "# print(\"Series of elder emotion scores:\", elder_emotion)\n",
        "\n",
        "# lets check the number of observations for each group!\n",
        "\n",
        "young_n = len(young_emotion)\n",
        "print(\"The number of young people are: \", young_n)\n",
        "\n",
        "elder_n = len(elder_emotion)\n",
        "print(\"The number of elder people are: \", elder_n)\n",
        "\n",
        "print() # extra space\n",
        "\n",
        "\n",
        "# second get sample means:\n",
        "\n",
        "# young sample mean:\n",
        "xbar_y = float(young_emotion.mean())\n",
        "print(\"Young emotions sample Mean:\", round(xbar_y,3))\n",
        "\n",
        "# male sample mean:\n",
        "xbar_e = float(elder_emotion.mean())\n",
        "print(\"Elder emotions sample Mean:\", round(xbar_e,3))\n",
        "\n",
        "\n",
        "print() # extra space\n",
        "\n",
        "# secondly filter the data to get the sample variances\n",
        "\n",
        "# female sample variance:\n",
        "s2_y = float(young_emotion.var())\n",
        "print(\"Young emotions sample Variance:\", round(s2_y,3))\n",
        "\n",
        "# male sample variance:\n",
        "s2_e = float(elder_emotion.var())\n",
        "print(\"Elder emotions sample Variance: \", round(s2_e,3))\n",
        "\n",
        "\n",
        "print()\n",
        "\n",
        "# Is one of the sample variances bigger than a factor of 3?\n",
        "\n",
        "print(\"Sample variance ratio:\", round(s2_y/s2_e,3) )\n",
        "\n"
      ],
      "metadata": {
        "id": "4HFFMTmgj2Rc"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "Since the sample variance ratio is 1.101 and not greater than 3, lets use the equal variance assumption for our A/B test!\n",
        "\n"
      ],
      "metadata": {
        "id": "U_F-J57joCPf"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "### p-value:"
      ],
      "metadata": {
        "id": "B44fs7ZaB20E"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "from scipy.stats import ttest_ind\n",
        "\n",
        "# 2 sample t-test code format:\n",
        "# ttest_ind(vector_A, vector_B, equal_var = True/False)\n",
        "\n",
        "# ALERT: do NOT do: ttest_ind(mean_A, mean_B, equal_var = True/False)\n",
        "# WE WANT DATA, NOT sample means!\n",
        "\n",
        "# t_obs -> the t-test statistic\n",
        "\n",
        "t_obs, p_value = ttest_ind(young_emotion, elder_emotion, equal_var = True )\n",
        "\n",
        "print(\"The t-test statistic:\", round(t_obs,3))\n",
        "print(\"The p-value: \", round(p_value,3))\n",
        "\n",
        "\n",
        "print()\n",
        "\n",
        "alpha_level_checker(p_value, alpha)\n"
      ],
      "metadata": {
        "id": "FXC_9XKeoVz2"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "### Conclusion and Potential Marketing Strategy:\n",
        "\n",
        "\n",
        "* Since $p-value = 0.043$ is less than $\\alpha = 0.05$, we reject the null hypothesis! (This is a close call though.)\n",
        "\n",
        "* We conclude that we do have enough evidence to say that average emotion scores for young people and and elder people are different!\n",
        "\n",
        "* Marketing Implication: A company should consider focus on selling products to older age groups. Their emotion scores appear higher than average, which could lead to **better ratings** for a company's products.\n",
        "\n",
        "\n",
        "\n",
        "---\n",
        "\n",
        "Now we have covered several A/B tests. But did you know that there is a way to do **A/B/C/D/E... or even larger tests**? Spooookkkyyyy ðŸ‘»!\n"
      ],
      "metadata": {
        "id": "6g2CzHvIB22p"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "# ðŸ”¢ SECTION G: ANOVA (A/B/C/D/E... Testing)\n",
        "\n",
        "You may wonder if we can extend to A/B testing to A/B/C testing,  A/B/C/D/E  testing, and so on.\n",
        "\n",
        "Well you are correct! We can extend the idea of A/B testing of two groups to multiple groups! We call this method ANOVA.\n",
        "\n",
        "**ANOVA** stands for: **AN**alysis **o**f **VA**riance\n",
        "\n",
        "\n",
        "\n",
        "\n",
        "* NOTE: ANOVA does NOT deal with proportions!\n",
        "\n",
        "\n",
        "---\n"
      ],
      "metadata": {
        "id": "TPbZKe4m9q5T"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "### If ANOVA is about testing different means, why in the world is it called Analysis of VARIANCE???\n",
        "\n",
        "This is a classic question. And once you understand the reasoning, the word ANOVA will make 200 percent sense!\n",
        "\n",
        "How can the means of groups be different? If each group have different distributions, then their means can be different.\n",
        "\n",
        "If we see a box and whisker plot, we can see that one group could have high centered annual income distribution, another group has a low centered annual income distribution, the third group has a moderately centered annual income distribution, and so on.\n",
        "\n",
        "If the group means are different from each other, then there is HIGH variance between the group means!\n",
        "\n",
        "We will see a example of a box whisker plot soon!\n",
        "\n",
        "---"
      ],
      "metadata": {
        "id": "o0x1_3tR-JNv"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "### Hypothesis Testing for ANOVA\n",
        "\n",
        "ANOVA Hypothesis Testing gets interesting. We test multiple means, BUT we just need at LEAST 1 mean to be different!\n",
        "\n",
        "ANOVA is used when we have 3 or more groups! (A/B testing is used when we have 2 groups, and single sample testing is used when we only have 1 group.)\n",
        "\n",
        "Suppose we are testing 5 groups:\n",
        "\n",
        "$$H_0: \\mu_A = \\mu_B =  \\mu_C = \\mu_D = \\mu_E$$\n",
        "\n",
        "$$H_a: \\text{At least one of the means are different!}$$\n",
        "\n",
        "\n",
        "You may think \"why are we not making the alternate hypothesis as $H_a: \\mu_A \\ne \\mu_B \\ne  \\mu_C \\ne  \\mu_D \\ne  \\mu_E$\"?\n",
        "\n",
        "Well the alternate hypothesis is anything different from the null hypothesis. And the alternate hypothesis wants to consider ALL different possibilities. So at least one mean being different considers ALL different possibilities!\n",
        "\n",
        "Why? Because the alternate hypothesis ($H_a$) is just saying\n",
        "that the null hypothesis ($H_0$) is NOT true. Thats it!\n",
        "\n"
      ],
      "metadata": {
        "id": "84F21rFJRbLJ"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "### ANOVA test statistic\n",
        "\n",
        "ANOVA uses an F-test. This comes from the F distribution.\n",
        "\n",
        "$$F_{obs} = \\frac{\\text{Variance between groups}}{\\text{Variance within groups}}$$\n",
        "\n",
        "\n",
        "\n",
        "1. Variance between groups:\n",
        "\n",
        "This is calculated by finding how far each group mean is from the overall mean of the data.  \n",
        "\n",
        "\n",
        "\n",
        "\n",
        "2. Variance within groups:\n",
        "\n",
        "This is when we simply we take the sample variance of each group. Then we try to take a WEIGHTED average all these sample variances.\n",
        "\n",
        "Sometimes the groups do not have the same number of observations, so averaging out all the sample variances requires weights.\n",
        "\n",
        "\n"
      ],
      "metadata": {
        "id": "i0X48LEQ-JFv"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "### ANOVA formula\n",
        "\n",
        "For this project, we will be focusing on one way ANOVA.\n",
        "\n",
        "Here is the one way ANOVA formula:\n",
        "\n",
        "\n",
        "$$Y_{ij} = \\mu + \\tau_i + \\epsilon_{ij}, \\text{ where } \\epsilon_{ij} \\sim N(0,\\sigma^2)$$\n",
        "\n",
        "Where:\n",
        "\n",
        "* $Y_{ij}$ is $j-th$ observation in group $i$.\n",
        "\n",
        "* $\\mu$ is the overall mean.\n",
        "\n",
        "* $\\tau_i$ is the effect of the $i-th$ group (the treatment effect).\n",
        "\n",
        "* $\\epsilon_{ij} \\sim N(0,\\sigma^2)$ is the random error.\n",
        "\n",
        "\n",
        "For example, lets say we want to measure the weight of 4 groups of athletes: football players, soccer players, basketball players, and golf players.\n",
        "\n",
        "* We can calculate their overall mean and make this the baseline value.\n",
        "\n",
        "* They each have different group means.\n",
        "\n",
        "* The group effect ($\\tau_i$) measures how each group mean deviates from the overall mean. The number can be positive or negative. If 0, the group mean does not deviate at all.\n",
        "\n",
        "This formula shows how ANOVA tests if group means are significantly different from each other or not.\n",
        "\n",
        "\n",
        "\n",
        "\n"
      ],
      "metadata": {
        "id": "f92ywGQc-JDM"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "## APPLICATION: ANOVA (Testing with 5 means of each country and their number of purchases)\n",
        "\n",
        "Recall our pie chart of the 10 countries.\n",
        "\n",
        "Lets do a ANOVA test on the top 5 countries that have the biggest proportions of customers."
      ],
      "metadata": {
        "id": "cTFpsSuo-JKO"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "fig = plt.figure(figsize = (15,12))\n",
        "\n",
        "\n",
        "color_list = [\"red\",\"blue\",\"green\",\"yellow\",\"pink\",\"orange\",\n",
        "              \"gold\",\"lightblue\", \"purple\", \"lightgreen\"]\n",
        "\n",
        "\n",
        "plt.pie(country_values,\n",
        "        labels = country_names,\n",
        "        colors = color_list,\n",
        "        autopct = \"%1.1f%%\",\n",
        "        explode = [0.2,0,0,0,0,0,0,0,0,0],\n",
        "        shadow = True\n",
        "\n",
        "        )\n",
        "\n",
        "plt.title(\"Comparing customer proportions of 10 different countries\", fontsize = 25);\n"
      ],
      "metadata": {
        "id": "I42HWvFE_A5G"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "Lets select these 5 countries:\n",
        "\n",
        "* USA\n",
        "* India\n",
        "* Brazil\n",
        "* UK\n",
        "* Germany\n",
        "\n",
        "Lets do a ANOVA test on these 5 countries and see if at least one mean value of number of purchases differs."
      ],
      "metadata": {
        "id": "TGjQ0nEm_IdC"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "### Hypothesis Test:\n",
        "\n",
        "Let us test:\n",
        "\n",
        "$H_0: \\mu_{USA} = \\mu_{India} = \\mu_{Brazil}= \\mu_{UK}= \\mu_{Germany}$\n",
        "\n",
        "$H_a: \\text{At least one of the means are different}$\n",
        "\n",
        "We need to get 5 series of data, the number of purchases for each country."
      ],
      "metadata": {
        "id": "yR2DDA5q-JXi"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "usa_series = AB_df[AB_df[\"country\"] == \"USA\"][\"num_purchases\"]\n",
        "\n",
        "india_series = AB_df[AB_df[\"country\"] == \"India\"][\"num_purchases\"]\n",
        "\n",
        "brazil_series = AB_df[AB_df[\"country\"] == \"Brazil\"][\"num_purchases\"]\n",
        "\n",
        "uk_series = AB_df[AB_df[\"country\"] == \"UK\"][\"num_purchases\"]\n",
        "\n",
        "germany_series = AB_df[AB_df[\"country\"] == \"Germany\"][\"num_purchases\"]\n",
        "\n",
        "\n",
        "# What do the means look like?\n",
        "\n",
        "print(\"Sample Mean of USA purchases:\", round(usa_series.mean(),3))\n",
        "print(\"Sample Mean of India purchases:\", round(india_series.mean(),3))\n",
        "print(\"Sample Mean of Brazil purchases:\", round(brazil_series.mean(),3))\n",
        "print(\"Sample Mean of UK purchases:\", round(uk_series.mean(),3))\n",
        "print(\"Sample Mean of Germany purchases:\", round(germany_series.mean(),3))\n",
        "\n"
      ],
      "metadata": {
        "id": "RnlxS8Xi_VKT"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "Brazil and India appear to have the lowest purchases, the other 3 countries have a similar amount of purchases.\n"
      ],
      "metadata": {
        "id": "1CiF9EtvNEQ6"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# ALERT: we need to do: .isin([])\n",
        "# This will NOT WORK: == \"USA\"|\"India\"|...\n",
        "five_df = AB_df[AB_df[\"country\"].isin([\"USA\",\"India\",\"Brazil\",\"UK\",\"Germany\"])]\n",
        "\n",
        "fig = plt.figure(figsize = (12,8))\n",
        "\n",
        "sns.boxplot(five_df, x = \"country\", y = \"num_purchases\", color = \"lightgreen\" )\n",
        "\n",
        "plt.xlabel(\"Country\")\n",
        "plt.ylabel(\"Number of Purchases\")\n",
        "\n",
        "plt.title(\"Boxplot of number of purchases for each country\", fontsize = 25);\n"
      ],
      "metadata": {
        "id": "1ZkqjyedNFBg"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "\n",
        "We can code ANOVA two ways:\n",
        "\n",
        "1. SciPy = quick code and quick results.\n",
        "\n",
        "2. statsmodels = more code but more results"
      ],
      "metadata": {
        "id": "mMkHSy2BCp00"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "### p-value:"
      ],
      "metadata": {
        "id": "FGxeToN996gf"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# Scipy way:\n",
        "\n",
        "from scipy.stats import f_oneway\n",
        "\n",
        "F_obs, p_value = f_oneway(usa_series, india_series,\n",
        "                          brazil_series, uk_series, germany_series)\n",
        "\n",
        "print(\"F test statistic value:\", round(F_obs,3))\n",
        "print(\"p-value:\", round(p_value,3))\n",
        "\n",
        "alpha_level_checker(p_value, alpha)\n"
      ],
      "metadata": {
        "id": "yB7DWhMvC1Sy"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# statsmodel way:\n",
        "\n",
        "import statsmodels.api as sm\n",
        "from statsmodels.formula.api import ols\n",
        "\n",
        "# make a df with only the 5 countries\n",
        "\n",
        "# ALERT: we need to do: .isin([])\n",
        "# This will NOT WORK: == \"USA\"|\"India\"|...\n",
        "five_df = AB_df[AB_df[\"country\"].isin([\"USA\",\"India\",\"Brazil\",\"UK\",\"Germany\"])]\n",
        "\n",
        "\n",
        "\n",
        "# fit the model:\n",
        "\n",
        "model = ols(\"num_purchases ~ C(country)\", data = five_df).fit()\n",
        "\n",
        "# typ = 2 -> tests each main effect, ignores interaction effects\n",
        "anova_table = sm.stats.anova_lm(model, typ = 2)\n",
        "\n",
        "anova_table"
      ],
      "metadata": {
        "id": "ZNguUoxKIHhU"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "Since p-value $\\approx$ 0 is less than $\\alpha = 0.05$, we have enough evidence to reject the null hypothesis!\n",
        "\n",
        "We have evidence that at least one mean number of purchases of these 5 countries are different.  \n",
        "\n",
        "Or do we???\n",
        "\n",
        "We need to check the assumptions of this ANOVA test."
      ],
      "metadata": {
        "id": "oyEY5ukrEMeV"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "## ANOVA Assumptions\n",
        "\n",
        "ANOVA has a few assumptions that we need to check.\n",
        "\n",
        "1. Normality: Are the residuals normal? A QQ plot can help detect this.\n",
        "\n",
        "2. Equal variance: Are the variances of the residuals of each group roughly similar? A boxplot can check this!\n",
        "\n",
        "3. Independence: Are the data points independent? AKA does one observation impact another observation?\n"
      ],
      "metadata": {
        "id": "39PZZyY3KZs4"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "### Normality Assumption:\n",
        "\n",
        "Are the residuals normally distributed?"
      ],
      "metadata": {
        "id": "cr9zb6XLKusg"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "residuals = model.resid\n",
        "# residuals\n",
        "\n",
        "# using this code does not work\n",
        "#fig = plt.figure(figsize = (15,8))\n",
        "\n",
        "# WARNING: need to assign a variable to only show one qqplot\n",
        "the_qqplot = sm.qqplot(residuals, line = \"45\")\n"
      ],
      "metadata": {
        "id": "oeHeTVJDKwu5"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "Well, it seems like the residuals are NOT normally distributed. The residuals should cluster around the qqline, but they are not.\n",
        "\n",
        "The Normality assumption seems violated.\n",
        "\n",
        "But how much observations do we have in total?"
      ],
      "metadata": {
        "id": "gh-QjrEALSjU"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "n_observations = len(five_df)\n",
        "print(\"The five countries have this much observations: \", n_observations)"
      ],
      "metadata": {
        "id": "ltc7YEJ-O61f"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "Since ANOVA is robust to non-normality with high a sample size, perhaps we can still use the hypothesis test result. We will need to check the other assumptions though.\n",
        "\n"
      ],
      "metadata": {
        "id": "f221qmthPIHT"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "###  Equal Variance Assumption:\n",
        "\n",
        "Lets make a boxplot to see if the variances are equal.\n",
        "\n",
        "\n"
      ],
      "metadata": {
        "id": "3rGxjmfhMEBl"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "five_df[\"residuals\"] = residuals\n",
        "\n",
        "fig = plt.figure(figsize = (12,8))\n",
        "\n",
        "sns.boxplot(five_df, x = \"country\", y = \"residuals\", color = \"darkred\")\n",
        "\n",
        "plt.xlabel(\"Country\")\n",
        "plt.ylabel(\"Number of Purchases\")\n",
        "\n",
        "plt.title(\"Box Plot of the residuals\", fontsize = 25);\n"
      ],
      "metadata": {
        "id": "8ZqjTAj2MXgZ"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "It appears that the variance assumption is met! The variances of the residuals seem constant for each country.\n"
      ],
      "metadata": {
        "id": "VjhlsGG-MvgE"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "###  Independence Assumption:\n",
        "\n",
        "We will assume that each observation is independent on each other. It makes sense that the first customer should not affect the characteristics of the second customer.\n",
        "\n",
        "If the first customer is rich while the second customer is middle class, the first customer is bound to buy more purchases, but the second customer is only buying a moderate amount of purchases because he is in middle class, not because he wants to be more humble than the first customer."
      ],
      "metadata": {
        "id": "OqfnwuNVNyRR"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "### Conclusion and Potential Marketing Strategy:\n",
        "\n",
        "* Even though ANOVA's normality assumption is violated, the high sample size of 6959 helps mitigate this issue.\n",
        "\n",
        "Also, the other assumptions appear to be met, so lets proceed with the hypothesis test results.\n",
        "\n",
        "* Since we rejected the null hypothesis, we  have evidence that at least one mean number of purchases of these 5 countries are different.  \n",
        "\n",
        "* So it appears that USA, India, Brazil, UK, and Germany do not share the same mean number for the number of purchases.\n",
        "\n",
        "\n",
        "* Marketing Implication: A company could focus on selling products on the USA, UK, and Germany since customers buy around 23 products on average, while customers on  Brazil and India buy around 21 products on average.\n",
        "\n",
        "Recall that the sample means for average products bought are:\n",
        "\n",
        "\n",
        "\n"
      ],
      "metadata": {
        "id": "eDyOeOnR96lc"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "print(\"Sample Mean of USA purchases:\", round(usa_series.mean(),3))\n",
        "print(\"Sample Mean of India purchases:\", round(india_series.mean(),3))\n",
        "print(\"Sample Mean of Brazil purchases:\", round(brazil_series.mean(),3))\n",
        "print(\"Sample Mean of UK purchases:\", round(uk_series.mean(),3))\n",
        "print(\"Sample Mean of Germany purchases:\", round(germany_series.mean(),3))\n"
      ],
      "metadata": {
        "id": "PkPM6G3KF_Ho"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "* **CAUTION:** Now a company should be careful. The average number of purchases does NOT always translate to more money. Maybe customer A buys 10 packs of Pokemon cards worth 50 USD while customer B buys a high end computer that is worth 3000 USD. Customer B clearly spent more money.\n",
        "\n",
        "* **Overall Insight:** Even if average number of purchases does not always mean more revenue, it is still a strong indicator that marketing in a country where customers tend to have many purchases is a good idea.\n",
        "\n"
      ],
      "metadata": {
        "id": "Y_0HYUBnGEv5"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "\n",
        "---\n",
        "\n",
        "Lets dive into a interesting concept called bootstrapping, a technique that lets us do inference even with a low sample size."
      ],
      "metadata": {
        "id": "rMWhVIq3GCa2"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "# ðŸ‘¢ SECTION H: Statistical Inference on Bootstrapping\n",
        "\n",
        "What if we do not have much observations? Are we going to give up and call it a day???\n",
        "\n",
        "NO! We have Bootstrap Bill to save the day! (Remember Bootstrap Bill from Pirates of the Carribean Dead Man's Chest, he was Davy Jone's right hand man!)\n",
        "\n",
        "\n",
        "In my Pokemon MEGA Project, I gave a great explanation of bootstrapping and I will show it here!\n",
        "\n",
        "\n",
        "\n",
        "\n"
      ],
      "metadata": {
        "id": "nBZi4lYCWnat"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "\n",
        "---\n",
        "\n",
        "## Bootstrapping Intuition:\n",
        "\n",
        "* Bootstrapping: This is when we have an original sample of data, then randomly pick observations (re-sample) from that original sample with replacement. The new sample is called the bootstrap sample.\n",
        "\n",
        "We do bootstrapping to mimic real-world data.\n",
        "\n",
        "* Now this might seem weird, why would I want to re-pick the same data, shouldn't I just get more data? Doesn't bootstrapping feel \"weird\", why am I allowed to use the same observation multiple times as real observations?\n",
        "\n",
        "* Well sometimes its expensive or just not possible to get more data, so this is why bootstrapping was made.\n",
        "\n",
        "* So what about the \"weird\" part? Well, MANY objects, animals, or people in real life contain the same elements. For example, let's say we have a sample of 50 students.\n",
        "\n",
        "Student 3 has blue eyes, is 6 feet tall, has yellow hair, and is American. Does that mean that student 3 is realistically the only person in the world who has blue eyes, is 6 feet, has yellow hair, and is American?\n",
        "\n",
        "NO! Why? Because in the real world,  there are other people in the world who have blue eyes, are 6 feet, have yellow hair, and are American! The fact that things, animals, or people have common traits is why bootstrapping is so powerful! Bootstrapping may select student 3 multiple times, and this is okay to do!\n",
        "\n",
        "\n",
        "* An example of bootstrapping: Say we have container 1 that contains 4 different candy bars that have different weights: 10g, 20g, 30g, 40g. A machine will replace any candy bar that is taken out of the container, and the candy bar's respective weight will be put in.\n",
        "\n",
        "The person closes his eyes and picks the candy bars at random.  The goal is to put 10 candy bars in container 2.\n",
        "\n",
        "Perhaps the person picks a 10g bar, he puts it at container 2, and the machine replaces the transferred 10g bar with another 10g bar. He does the process 9 more times. The process soon goes on and container 2 has 5 10g bars, 4 20g bars, 1 30g bar, and 0 40g bars.\n",
        "\n",
        "Random re-sampling happened when the person closed his eyes and picked a candy bar from container 1 and moved it to container 2.  Replacement happened when the machine replaced the candy bar. The bootstrap sample is container 2.\n",
        "\n",
        "Notice that some candy bar weights are picked multiple times due to replacement, and 40g bars are not picked at all since there is a rare chance a certain type of candy bar may not even be picked.\n",
        "\n",
        "(This explenation originally appeared in my  Pokemon Mega Project.)\n",
        "\n",
        "---\n",
        "\n"
      ],
      "metadata": {
        "id": "vI2CO3bxYhOr"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "### What's different from normal data and bootstrapped data?\n",
        "\n",
        "Now, statistical inference is wayyyyyyy different when using boostrapped data. The interpretation of hypothesis tests, confidence intervals, rejection regions, and p-values is different from regular data.\n",
        "\n",
        "\n",
        "---\n"
      ],
      "metadata": {
        "id": "pFFZgQQrY_79"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "## â— WARNING: We do NOT use bootstrapping to \"increase the sample size\"!\n",
        "\n",
        "Bootstrapping is NOT used to inflate the sample size or to create fake users!\n",
        "\n",
        "The point of bootstrapping is to find distrubution properties such:\n",
        "\n",
        "* bootstrap confidence intervals\n",
        "\n",
        "* bootstrap hypothesis tests\n",
        "\n",
        "* variablity of a statistic\n",
        "\n",
        "Bootstrapping only helps us understand big picture ideas such as the behavior of our estimates. Bootstrapping does NOT give additional evidence."
      ],
      "metadata": {
        "id": "O3tKdgYQZeUu"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "## Bootstrapping Process\n",
        "\n",
        "Suppose we have a vector of  data: $\\vec x = [x_1, x_2,...,x_k]$\n",
        "\n",
        "1. Generate a bootstrap sample.\n",
        "\n",
        "Generate one bootstrap sample by using bootstrapping on $\\vec x$.\n",
        "\n",
        "We will soon end up with the first bootstrap sample: $\\vec b_1 = [x_1^*, x_2^*,...,x_n^*]$\n",
        "\n",
        "We then calculate a statistic such as the mean. We will use take the mean of the bootstrap sample: $\\bar b_1 = \\frac{1}{n} \\sum^n_{i=1} x_i^*$\n",
        "\n",
        "2. Generate more bootstrap samples\n",
        "\n",
        "Repeat step 1 for $B$ times. $B$ is the number of bootstrap samples we want.\n",
        "\n",
        "Each cycle will give us new bootstrap means: $\\bar b_2, \\bar b_3,..., \\bar b_B$\n",
        "\n",
        "Usually, the minimum value of $B$ is: $B = 1000$\n",
        "\n",
        "3. We have our bootstrap distribution.\n",
        "\n",
        "After step 2, we should have a bootstrap distribution.  We also  should have a collection of bootstrap means:\n",
        "\n",
        "$$\\vec b = [\\bar b_1, \\bar b_2,...,\\bar b_B]$$\n",
        "\n",
        "We can make a histogram to see the bootstrap distribution.\n",
        "\n",
        "\n",
        "4. Take the overall bootstrap mean.\n",
        "\n",
        "Lets take one more mean. This can estimate the population mean $\\mu$ if the original sample mean $\\bar x$ does not have much estimators.\n",
        "\n",
        "$b_{overall}  = \\frac{1}{B} \\sum^B_{i=1} \\bar b_i$\n",
        "\n",
        "\n",
        "$\\widehat \\mu = b_{overall}$ is an estimator for $\\mu$.\n",
        "\n",
        "---\n",
        "\n"
      ],
      "metadata": {
        "id": "GPsFcL3TZeWy"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "## Bootstrapping Process for a difference in means\n",
        "\n",
        "Suppose we have two vectors of  data, group A and group B: $\\vec x_A = [x_{1A}, x_{2A},...,x_{kA}]$\n",
        "\n",
        "$\\vec x_B = [x_{1B}, x_{2B},...,x_{kB}]$\n",
        "\n",
        "1. Generate a bootstrap sample.\n",
        "\n",
        "Generate one bootstrap sample by using bootstrapping on $\\vec x_A$ and $\\vec x_B$.\n",
        "\n",
        "We will soon end up with the first bootstrap samples: $ \\vec x_A^* = [x_{1A}^*, x_{2A}^*,...,x_{nA}^*]$ and $ \\vec x_B^* = [x_{1B}^*, x_{2B}^*,...,x_{nB}^*]$\n",
        "\n",
        "Usually $n_A$ and  $n_B$ are equal to $k$.\n",
        "\n",
        "\n",
        "We will use take the mean of the bootstrap samples: $\\bar x_A^* = \\frac{1}{n_A} \\sum^{n_A}_{i=1} x_{iA}^*$\n",
        "\n",
        "$\\bar x_B^* = \\frac{1}{n_B} \\sum^{n_B}_{i=1} x_{iB}^*$\n",
        "\n",
        "We then calculate the difference in means statistic. This is our bootstrap statistic:\n",
        "\n",
        "$$\\bar b_1 = \\bar x_A^* - \\bar x_B^*$$\n",
        "\n",
        "\n",
        "\n",
        "\n",
        "2. Generate more bootstrap samples\n",
        "\n",
        "Repeat step 1 for $B$ times. $B$ is the number of bootstrap samples we want.\n",
        "\n",
        "Each cycle will give us new bootstrap means: $\\bar b_2, \\bar b_3,..., \\bar b_B$\n",
        "\n",
        "Usually, the minimum value of $B$ is: $B = 1000$\n",
        "\n",
        "3. We have our bootstrap distribution.\n",
        "\n",
        "After step 2, we should have a bootstrap distribution.  We also  should have a collection of bootstrap means:\n",
        "\n",
        "$$\\vec b = [\\bar b_1, \\bar b_2,...,\\bar b_B]$$\n",
        "\n",
        "We can make a histogram to see the bootstrap distribution.\n",
        "\n",
        "\n",
        "4. Take the overall bootstrap mean.\n",
        "\n",
        "Lets take one more mean. This can estimate the population mean $\\mu$ if the original sample mean $\\bar x$ does not have much estimators.\n",
        "\n",
        "$b_{overall}  = \\frac{1}{B} \\sum^B_{i=1} \\bar b_i$\n",
        "\n",
        "\n",
        "$\\widehat \\mu_A - \\widehat \\mu_B = b_{overall}$ is an estimator for $\\mu_A - \\mu_B$.\n",
        "\n",
        "----\n"
      ],
      "metadata": {
        "id": "4DV2WUX7M1Jb"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "### Hypothesis Test for bootstrapped data when we have difference in means\n",
        "\n",
        "We would usually test:\n",
        "\n",
        "$$H_0: \\mu_A - \\mu_B = 0$$\n",
        "\n",
        "$$H_a: \\mu_A - \\mu_B \\ne 0$$\n",
        "\n",
        "But we calculating the test statistics and p-values can be a bit tedious with bootstrapped data.\n",
        "\n",
        "(The same dilemma happens with difference in two proportions.)\n"
      ],
      "metadata": {
        "id": "bFeS2AQyOjFW"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "### Calculating test statistics and p-values with Bootstrapped data:\n",
        "\n",
        "When calculating the test statistic and p-values, we would need to center the data.\n",
        "\n",
        "1. For every original observation:\n",
        "\n",
        "$x_i^{centered} = x_i - \\bar x + \\mu_0$\n",
        "\n",
        "2. Do the bootstrap process.\n",
        "\n",
        "\n",
        "\n",
        "* So why in the world do we have to center each observation? Well it is because the bootstrapped data is centered on $\\bar x$, NOT $\\mu_0$.\n",
        "\n",
        "* So why did we not center each observation for our previous t-tests or AB tests? Well it is because the the test statistic already subtracted the sample mean $\\bar x$ by $\\mu_0$! Recall that the two sample t-test for unequal variances is:\n",
        "\n",
        "\n",
        "$$t_{obs} = \\frac{\\bar x_A - \\bar x_B}{\\sqrt{\\frac{s^2_A}{n_A} + \\frac{s^2_B}{n_B} } }$$\n",
        "\n",
        "\n",
        "---\n",
        "\n",
        "Can we avoid this centering condition!\n",
        "\n",
        "YES! We just need to invite back our old friend, the confidence interval. And with bootstrapped data, the confidence interval is looking different!\n"
      ],
      "metadata": {
        "id": "FdVYIXkUPUov"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "### Confidence Intervals for Bootstrapped data\n",
        "\n",
        "Our old friend the Confidence Interval (CI) is more easy going when he is in the bootstrapped mode.\n",
        "\n",
        "A $(1-\\alpha)%$ bootstrapped Confidence Interval is extremely easy to compute. Here it is:\n",
        "\n",
        "$$(\\frac{\\alpha}{2} \\text{ quantile}, 1- \\frac{\\alpha}{2} \\text{ quantile})$$\n",
        "\n",
        "\n",
        "\n",
        "**THATS IT!** We just take percentiles and call it a day for bootstrap CIs!\n",
        "\n",
        "Why can bootstrap CIs be this simple? It is because bootstrap distributions does NOT assume any specific population distribution!\n",
        "\n",
        "\n",
        "* ALERT: We do NOT need to center the data when making a bootstrap CI. Why?\n",
        "\n",
        "Because a CI just gives a range of possible parameter values. It does not make p-values, and p-values need centered data!\n",
        "\n",
        "* Just like regular hypothesis testing, if the $H_0$ value is contained in  the bootstrap CI, we fail to reject the null hypothesis. If the $H_0$ value is outside  the bootstrap CI, then reject the null hypothesis!\n",
        "\n",
        "---"
      ],
      "metadata": {
        "id": "OINcYE-aZeZV"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "# ðŸ« SECTION I: A/B testing on Bootstrapped data\n",
        "\n",
        "\n",
        "\n",
        "\n",
        "* NOTE: Bootstrapping can work with proportions!\n"
      ],
      "metadata": {
        "id": "W8Ykgb6vZeeP"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "## APPLICATION: A/B Testing on Two Means (Bootstrapped data on Male and Females annual income):\n",
        "\n",
        "Recall in Section E that we did a regular A/B test on a difference in means. It was about the annual income of males and females.\n",
        "If you recall the hypothesis test, we failed to reject the null and concluded we do not have enough evidence to say that males and females have different annual incomes.\n",
        "\n",
        "What happens if we only had a small amount of data from these males and females? Will bootstrapped data give similar hypothesis test results of failing to reject the null hypothesis?\n"
      ],
      "metadata": {
        "id": "dcH2860FZlI1"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "male_female_df = AB_df.groupby([\"gender\"]).mean(numeric_only = True)\n",
        "\n",
        "male_female_df\n",
        "\n",
        "\n",
        "\n"
      ],
      "metadata": {
        "id": "enX_7y3dZ0Qi"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# first filter the data to get female and male groups:\n",
        "\n",
        "female_income = AB_df[AB_df[\"gender\"] == \"Female\"][\"annual_income\"]\n",
        "print(\"Series of female income:\", female_income)\n",
        "\n",
        "print()\n",
        "\n",
        "male_income = AB_df[AB_df[\"gender\"] == \"Male\"][\"annual_income\"]\n",
        "print(\"Series of male income:\", male_income)\n"
      ],
      "metadata": {
        "id": "0jZInbaWfDG4"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "### Hypothesis Test:\n",
        "\n",
        "Lets test the annual_income means of:\n",
        "\n",
        "$$H_0: \\mu_{Female} - \\mu_{Male} = 0$$\n",
        "\n",
        "$$H_a: \\mu_{Female} - \\mu_{Male} \\ne 0$$\n",
        "\n",
        "\n",
        "ALERT: We are not trying to find a test statistic, we are trying to make a bootstrap CI. We will see if\n",
        "\n",
        "\n",
        "Lets also set our alpha level to be: $\\alpha = 0.05$\n",
        "\n",
        "With this alpha level, we will make a $(1-\\alpha)\\% \\text{ CI} = (1- 0.05)\\% \\text{ CI} = (0.95)\\% \\text{ CI}$\n",
        "\n",
        "---\n"
      ],
      "metadata": {
        "id": "WhfGDAPyZlLJ"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "### Intial small data samples PLUS bootstrapping process\n",
        "\n",
        "Lets do two parts.\n",
        "\n",
        "NOTE: We will be using a LOT of the numpy library to do bootstrapping!\n",
        "\n",
        "**Part 1: Small data samples**\n",
        "\n",
        "Lets randomly sample 20 observations from the males and females series. This is our small data series.\n",
        "\n",
        "**Part 2: Bootstrapping process**\n",
        "\n",
        "Then lets bootstrap those small data series. Lets make about 1000 bootstrap statistics of: $\\bar b_1 = \\bar x_A^* - \\bar x_B^*$\n"
      ],
      "metadata": {
        "id": "8zx79Qq_bY1_"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# part 1: small data samples\n",
        "\n",
        "# seed number is 1:\n",
        "rng = np.random.default_rng(1)\n",
        "\n",
        "#rng.choice() is our sampling function!\n",
        "\n",
        "small_females = rng.choice(female_income, 20)\n",
        "print(\"Our random sample of 20 females:\", small_females)\n",
        "\n",
        "print()\n",
        "\n",
        "small_males = rng.choice(male_income, 20)\n",
        "print(\"Our random sample of 20 males:\", small_males)\n",
        "\n",
        "\n",
        "print() # extra space\n",
        "\n",
        "\n",
        "\n",
        "# part 2: bootstrapping process\n",
        "\n",
        "# lets make 1000 bootstrap statistics\n",
        "B = 1000\n",
        "\n",
        "# make a empty list:\n",
        "bootstrap_statistic_list = []\n",
        "\n",
        "\n",
        "\n",
        "# FOR loop -> CONTAINS MOST OF THE BOOTSTRAP PROCESS!\n",
        "\n",
        "# lets select 1000 bootstrap sample for each bootstrap!\n",
        "\n",
        "for x in range(1, B + 1):\n",
        "  boot_females = rng.choice(small_females, 20)\n",
        "  boot_female_mean = np.mean(boot_females)\n",
        "\n",
        "  boot_males = rng.choice(small_males, 20)\n",
        "  boot_male_mean = np.mean(boot_males)\n",
        "\n",
        "  boot_statistic = boot_female_mean - boot_male_mean\n",
        "\n",
        "  bootstrap_statistic_list.append(boot_statistic)\n",
        "\n",
        "\n",
        "\n",
        "print(\"First 5 observations in the bootstrap distribution is:\",\n",
        "      bootstrap_statistic_list[0:5])\n",
        "\n",
        "print(\"Overall Bootstrap Mean value is:\",\n",
        "      round(np.mean(bootstrap_statistic_list),3))\n"
      ],
      "metadata": {
        "id": "0CKCWqD8bSZF"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "Lets see a histogram of our bootstrap distribution!"
      ],
      "metadata": {
        "id": "vVSwfYYljQgb"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "fig = plt.figure(figsize = (15,8))\n",
        "\n",
        "plt.hist(x = bootstrap_statistic_list, bins = 20, color = \"lightgreen\")\n",
        "\n",
        "plt.ylabel(\"Count\", fontsize = 20)\n",
        "\n",
        "plt.xlabel(\"Bootstrap statistics\", fontsize = 20)\n",
        "\n",
        "plt.title(\"Bootstrap Distribution \", fontsize = 30);\n",
        "\n"
      ],
      "metadata": {
        "id": "jvoN2aZ9jQFK"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "### Bootstrap Confidence Interval:\n",
        "\n",
        "Recall that a $(1-\\alpha)%$ bootstrapped Confidence Interval is extremely easy to compute.\n",
        "\n",
        "$$(\\frac{\\alpha}{2} \\text{ quantile}, 1- \\frac{\\alpha}{2} \\text{ quantile})$$\n",
        "\n",
        "Lets make a bootstrap CI using NumPy."
      ],
      "metadata": {
        "id": "PIlgJf15-No9"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "\n",
        "# ALERT: BE CAREFUL!!!\n",
        "# np.perentile(series/list, PERCENT (0% to 100%))\n",
        "\n",
        "# WARNING: doing np.perentile(series/list, 0.025) gives 0.025%, this is NOT good!\n",
        "# do: np.percentile(series/list, 2.5), this gives 2.5%!!! This is what we want!\n",
        "# can also do 0.025 * 100 = 2.5\n",
        "\n",
        "# bootstrap lower limit:\n",
        "LL = np.percentile(bootstrap_statistic_list, (alpha/2) * 100)\n",
        "LL = round(float(LL),3)\n",
        "\n",
        "# bootstrap upper limit:\n",
        "UL = np.percentile(bootstrap_statistic_list, (1-alpha/2) * 100)\n",
        "UL = round(float(UL),3)\n",
        "\n",
        "# make a tuple for the bootstrap CI\n",
        "bootstrap_CI = (LL,UL)\n",
        "\n",
        "print(\"The bootstrap CI is:\", bootstrap_CI)\n"
      ],
      "metadata": {
        "id": "ho2_BYN-llUY"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "### Bootstrap distribution AND Bootstrap CI plot"
      ],
      "metadata": {
        "id": "3dKrVu-oOpJi"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "fig = plt.figure(figsize = (15,8))\n",
        "\n",
        "plt.hist(x = bootstrap_statistic_list, bins = 20, color = \"lightgreen\")\n",
        "\n",
        "plt.ylabel(\"Count\", fontsize = 20)\n",
        "\n",
        "plt.xlabel(\"Bootstrap statistics\", fontsize = 20)\n",
        "\n",
        "# lines for the 95% bootstrap CI\n",
        "plt.axvline(x = -11031.764, color = \"blue\", label = \"The 95% bootstrap CI, notice that 0 is inside it!\")\n",
        "plt.axvline(x = 33794.43, color = \"blue\")\n",
        "\n",
        "plt.axvline(x = 0, color = \"red\", linestyle = \"--\", label = \"The null value. Recall that it is 0.\")\n",
        "\n",
        "plt.legend()\n",
        "\n",
        "plt.title(\"Bootstrap Distribution and the 95% Bootstrap CI\", fontsize = 30);\n",
        "\n"
      ],
      "metadata": {
        "id": "oykCPrQzonGr"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "Notice that the null value of 0 is **contained inside** the bootstrap CI.\n",
        "\n",
        "Since it is possible that the actual paramter value of $\\mu_{female} - \\mu_{male}$ is actually 0, we do not have enough evidence to reject the null hypothesis.\n",
        "\n",
        "We do not have enough evidence to conclude that men and females have different annual incomes, EVEN with bootstrapping!\n",
        "\n",
        "And to interpret this bootstrap CI: We are 95% confident that this bootstrap CI of  (-11031.764, 33794.43) contains the parameter value.\n",
        "\n"
      ],
      "metadata": {
        "id": "4qbntw4ln6mW"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "### Conclusion and Potential Marketing Strategy:\n",
        "\n",
        "* So the bootstrap CI contained the null value and because of it, we failed to reject the null hypothesis.\n",
        "\n",
        "* So Section E of regular A/B testing and Section I of bootstrap A/B testing gave similar results. That was interesting since bootstrap used small amounts of data and yet we failed to reject the null hypothesis in both A/B tests!\n",
        "\n",
        "\n",
        "* Marketing Implication: Again, just like Section E, a company should NOT focus on targeting a specific gender to maximize profits. It does not seem like a company will get significantly more revenue if we simply target male customers. The Bootstrap experiment strengthens the conclusion that  targeting specific genders to is unlikely to increase financial gain substantially.\n",
        "\n",
        "\n",
        "\n",
        "\n",
        "\n",
        "---"
      ],
      "metadata": {
        "id": "zftF-McNZlNL"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "# ðŸ’¡ SECTION J: Final Conclusion\n"
      ],
      "metadata": {
        "id": "fMHSe5HRZlPc"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "Here is a table that summarizes all the 6 statistical tests we did:\n",
        "\n"
      ],
      "metadata": {
        "id": "a93BKnv8Zegd"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# Make df display all content in columns:\n",
        "pd.set_option(\"display.max_colwidth\", None)\n",
        "\n",
        "summary_table = pd.DataFrame({\n",
        "\n",
        " \"Statistical Test\": [\"Single Mean Test on annual income for USA customers:\",\n",
        "                      \"Difference in Means A/B Test on annual income for Males and Females:\",\n",
        "                      \"Difference in Proportions A/B Test on churned customers for USA and other countries:\",\n",
        "                      \"Difference in Means A/B Test on feedback text for young and elder people (Used NLP):\",\n",
        "                      \"One Way ANOVA Test on the average number of purchases made for each of the 5 countries:\",\n",
        "                      \"Difference in Means A/B Test on annual income for Males and Females (Bootstrap Resampling to revisit Statistical Test 2):\",\n",
        "],\n",
        "\n",
        "\n",
        "\"Results\":[\"Rejected the null that the average income is 67080. BUT due to sampling bias, this conclusion is not reliable.\",\n",
        "           \"Failed to reject the null. Concluded that we do not have enough evidence that Males and Females have different average annual incomes.\",\n",
        "           \"Failed to reject the null. Concluded that we do not have enough evidence that the churn proportions of the USA and other countries are different.\",\n",
        "           \"Rejected the null. Concluded we DO have enough evidence that young and elder people have different average emotional scores.\",\n",
        "           \"Rejected the null. Concluded that we DO have enough evidence that at LEAST 1 country has a different average number of purchases compared to other countries.\",\n",
        "           \"Failed to reject the null. Even Bootstrapped data had the same conclusion as the original A/B Test on annual income for Males and Females.\"],\n",
        "\n",
        "\"Evidence\":[\"The p-value is approximately 0. But we can not trust fully this p-value due to possible sampling bias.\",\n",
        "            \"The p-value = 0.35. This is greater than alpha = 0.05, so we fail to reject the null.\",\n",
        "            \"The p-value = 0.80. This is greater than alpha = 0.05, so we fail to reject the null.\",\n",
        "            \"The p-value = 0.043. This is LESS than alpha = 0.05, so we reject the null.\",\n",
        "            \"The p-value is approximately 0. This is LESS than alpha = 0.05, so we reject the null.\",\n",
        "            \"The bootstrap Confidence Interval contains the null mean value of 0. Therefore, we fail to reject the null.\"],\n",
        "\n",
        "\"Marketing Suggestion\":[\"No suggestion. Sampling bias might be present, so any suggestion may not be reliable.\",\n",
        "                        \"A company should NOT focus on targeting a specific gender to maximize revenue.\",\n",
        "                        \"USA companies should NOT focus on targeting a non-USA countries to avoid high churn rates.\",\n",
        "                        \"A company should consider focus on selling products to older age groups!\",\n",
        "                        \"The Marketing Team COULD prioritize selling products on the USA, UK, and Germany.\",\n",
        "                        \"For statistical test 2, we have more insights that a company should NOT focus on targeting a specific gender to maximize revenue.\"]\n",
        "\n",
        "\n",
        " })\n",
        "\n",
        "summary_table.index = range(1, 6 + 1)\n",
        "\n",
        "\n",
        "summary_table\n"
      ],
      "metadata": {
        "id": "pedBclZarFfy"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# reset the column settings:\n",
        "pd.reset_option(\"display.max_colwidth\")"
      ],
      "metadata": {
        "id": "-sJJf9WnzdFR"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "We can see that we will not always reject the null hypothesis. In fact, it is actually common to fail to reject the null hypothesis!\n",
        "\n",
        "We also need to use our judgement. Just because we have a small p-value, it does not always mean it is reliable!!!\n",
        "\n",
        "\n"
      ],
      "metadata": {
        "id": "1lN2FyTbZqR3"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "### How powerful are statistical inference tests?\n",
        "\n",
        "So in the tests where we rejected the null hypothesis, can we say that we know what the population means are with certainity?\n",
        "\n",
        "NO! Single Mean tests, A/B tests, and ANOVA can ONLY tell us that the single group is not that population mean or that multiple groups do not have similar population means or proportions. Thats it!\n",
        "\n",
        "We made a null hypothesis of $H_0: \\mu = 30$, a alternative hypothesis of $H_a: \\mu \\ne 30$, and a p-value of 0.00132, we can NOT say: \"I believe the population mean is 45\".\n",
        "\n",
        "A statistical test is NOT that powerful! It can not give certain population parameter numbers! But it can tell people if there is evidence if a population parameter is likely another number.\n"
      ],
      "metadata": {
        "id": "6CF2Bo8-4r-8"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "### Why do Statistical Tests in the first place?\n",
        "\n",
        "So if a statistical test can not give us certain parameter numbers, why do many financial companies, health care agencies, sports teams, and anyone do a hypothesis test?\n",
        "\n",
        "Well hypothesis tests can provide evidence that a certain group is different from another group.\n",
        "\n",
        "Recall what I said in Section A:\n",
        "\n",
        "\"Lets say that conduct did a A/B test on the weights of NBA centers vs NBA point guards. We find that we have evidence that the weights of centers and point guards are different.\n",
        "\n",
        "We can use that decision to make basketball strategies where a center who tends to be heavier should prioritize being in the paint, while a point guard should do plays that require fast movement since point guards tend to weigh less.\n",
        "\n",
        "Statistical tests like t-tests, A/B tests, and ANOVA are powerful tools for data-driven decision making. This project shows some reasons why they are widely used in data science today.\"\n",
        "\n",
        "\n",
        "So even if we can not pin point exact parameter values, we can still use results from statistical tests to make a decision!\n",
        "\n",
        "**If you were to do a A/B test, what groups would you want to compare?**\n"
      ],
      "metadata": {
        "id": "dL7KItJx54pR"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "# Bibliography\n",
        "\n",
        "Kacem, Fares Ben. â€œCustomers Transactions.â€ Kaggle, 31 Oct. 2025, www.kaggle.com/datasets/fares279/customers-transactions/data. Accessed 7 Dec. 2025.\n",
        "\n",
        "IBM. â€œWhat Is Exploratory Data Analysis?â€ IBM, 17 Nov. 2025, www.ibm.com/think/topics/exploratory-data-analysis#:~:text=data%20analysis%20languages-,What%20is%20EDA?,the%20data%20discovery%20process%20today. Accessed 8 Dec. 2025.\n",
        "\n",
        "â€œGDP by Country.â€ Worldometer, www.worldometers.info/gdp/gdp-by-country/. Accessed 8 Dec. 2025.\n",
        "\n",
        "\n",
        "â€œMean Personal Income in the United States.â€ FRED, 9 Sept. 2025, fred.stlouisfed.org/series/MAPAINUSA646N. Accessed 8 Dec. 2025.\n",
        "\n",
        "NLP Explained - AWS, aws.amazon.com/what-is/nlp/. Accessed 10 Dec. 2025.\n"
      ],
      "metadata": {
        "id": "lqTjTUsuZqbn"
      }
    }
  ]
}